#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
webhook_main.py ‚Äî —á–∏—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è (–≥–æ—Ç–æ–≤–∞—è –∫ –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–µ)
- AIOHTTP –≤–µ–±-—Å–µ—Ä–≤–µ—Ä –ø–æ–¥ Telegram Webhook
- –ì–∏–±—Ä–∏–¥–Ω—ã–π retrieve: —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ + BM25/–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
- –ñ—ë—Å—Ç–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ embeddings.npy ‚Üî JSON
- –°—Ç—Ä–æ–≥–∏–π JSON-–≤—ã–≤–æ–¥ –æ—Ç LLM –∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–µ–Ω–¥–µ—Ä –≤ HTML –¥–ª—è Telegram
- –ë–µ–∑ –¥—É–±–ª–µ–π —Ñ—É–Ω–∫—Ü–∏–π –∏ "–º–∞–≥–∏—á–µ—Å–∫–∏—Ö" –∫–æ–Ω—Å—Ç–∞–Ω—Ç –≤ –ª–æ–≥–∏–∫–µ

–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (requirements.txt):
    aiohttp
    requests
    numpy
    openai>=1.30.0
    rank_bm25     # —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è, –Ω–æ –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ (fallback –Ω–∞ keyword-—Å–∫–æ—Ä–∏–Ω–≥)
    # (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –¥–ª—è –ª–æ–≥–æ–≤ –≤ Google Sheets)
    gspread
    google-auth
    google-auth-oauthlib
    google-auth-httplib2

–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ):
    OPENAI_API_KEY
    TELEGRAM_TOKEN
    WEBHOOK_URL                   # –Ω–∞–ø—Ä–∏–º–µ—Ä, https://your-app.onrender.com

–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ/–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ):
    PORT                          # default: 8080
    WEBHOOK_PATH                  # default: /webhook
    TELEGRAM_WEBHOOK_SECRET       # —Å–µ–∫—Ä–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ X-Telegram-Bot-Api-Secret-Token
    EMBEDDINGS_PATH               # default: embeddings.npy
    PUNKTS_PATH                   # default: pravila_detailed_tagged_autofix.json
    EMBEDDING_MODEL               # default: text-embedding-ada-002 (—Å–æ–≤–º–µ—Å—Ç–∏–º —Å —Ç–µ–∫—É—â–∏–º embeddings.npy)
    CHAT_MODEL                    # default: gpt-4o-mini
    MULTI_QUERY                   # "1" ‚Üí –≤–∫–ª—é—á–∏—Ç—å –ú—É–ª—å—Ç–∏-–ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ (–¥–æ—Ä–æ–∂–µ)
    SHEET_ID, GOOGLE_CREDENTIALS_JSON  # –µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã/–æ—Ç–≤–µ—Ç—ã –≤ Google Sheets

–ê–≤—Ç–æ—Ä: —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –∑–∞–º–µ–Ω—ã —Å—Ç–∞—Ä–æ–≥–æ webhook_main.py, –±–µ–∑ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π.
"""
from __future__ import annotations

import os
import json
import time
import html
import uuid
import logging
import asyncio
import re
import threading  # NEW

from typing import List, Dict, Any, Tuple, Optional

import numpy as np
import requests
from aiohttp import web
from collections import OrderedDict  # NEW

# OpenAI v1 style
from openai import OpenAI
from openai import APIStatusError, APIConnectionError, RateLimitError, APITimeoutError


# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è RU
try:
    import pymorphy2  # type: ignore
    _MORPH = pymorphy2.MorphAnalyzer()
except Exception:
    _MORPH = None

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: BM25
try:
    from rank_bm25 import BM25Okapi
    HAVE_BM25 = True
except Exception:
    HAVE_BM25 = False

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –∏–º–ø–æ—Ä—Ç —Å–ª–æ–≤–∞—Ä–µ–π –º–∞–ø–ø–∏–Ω–≥–æ–≤ (–µ—Å–ª–∏ —Ñ–∞–π–ª –µ—Å—Ç—å –∏ –≤–∞–ª–∏–¥–µ–Ω)
try:
    import importlib.util
    _cfg_path = os.path.join(os.getcwd(), "config_maps.py")
    if os.path.exists(_cfg_path):
        spec = importlib.util.spec_from_file_location("config_maps", _cfg_path)
        cfg = importlib.util.module_from_spec(spec) if spec else None
        if spec and cfg and spec.loader:
            spec.loader.exec_module(cfg)  # type: ignore
            UNIVERSAL_MAP: Dict[str, Dict[str, str]] = getattr(cfg, "UNIVERSAL_MAP", {})
        else:
            UNIVERSAL_MAP = {}
    else:
        UNIVERSAL_MAP = {}
except Exception:
    UNIVERSAL_MAP = {}

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ö–æ–Ω—Ñ–∏–≥ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

TELEGRAM_TOKEN = os.environ.get("TELEGRAM_TOKEN", "").strip()
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "").strip()
WEBHOOK_URL = os.environ.get("WEBHOOK_URL", "").strip()

assert TELEGRAM_TOKEN, "TELEGRAM_TOKEN is required"
assert OPENAI_API_KEY, "OPENAI_API_KEY is required"
assert WEBHOOK_URL, "WEBHOOK_URL is required (e.g., https://your-app.onrender.com)"

PORT = int(os.environ.get("PORT", "8080"))
WEBHOOK_PATH = os.environ.get("WEBHOOK_PATH", "/webhook")
TELEGRAM_WEBHOOK_SECRET = os.environ.get("TELEGRAM_WEBHOOK_SECRET", "").strip() or None
# sanitize WEBHOOK_PATH/WEBHOOK_URL
if not WEBHOOK_PATH.startswith("/"):
    WEBHOOK_PATH = "/" + WEBHOOK_PATH
WEBHOOK_URL = WEBHOOK_URL.rstrip("/")
EMBEDDINGS_PATH = os.environ.get("EMBEDDINGS_PATH", "embeddings.npy")
PUNKTS_PATH = os.environ.get("PUNKTS_PATH", "pravila_detailed_tagged_autofix.json")

# –í–ù–ò–ú–ê–ù–ò–ï: embeddings.npy —Å–µ–π—á–∞—Å –Ω–∞ 1536-–º–µ—Ä–Ω–æ–π –º–æ–¥–µ–ª–∏ ada-002 ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–∞–∫—É—é –∂–µ –º–æ–¥–µ–ª—å –¥–æ –ø–µ—Ä–µ—Å—á—ë—Ç–∞
EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL", "text-embedding-3-large")

CHAT_MODEL = os.environ.get("CHAT_MODEL", "gpt-4o-mini")


MULTI_QUERY = os.environ.get("MULTI_QUERY", "0") == "1"
HYDE = os.environ.get("HYDE", "0") == "1"  # ‚¨ÖÔ∏è –¥–æ–±–∞–≤–∏—Ç—å —ç—Ç—É —Å—Ç—Ä–æ–∫—É
LLM_RERANK = os.environ.get("LLM_RERANK", "1") == "1"


SHEET_ID = os.environ.get("SHEET_ID", "").strip() or None
GOOGLE_CREDENTIALS_JSON = os.environ.get("GOOGLE_CREDENTIALS_JSON", "").strip() or None

# –°–∫–æ—Ä–∏–Ω–≥–æ–≤—ã–µ –≤–µ—Å–∞
W_EMB = 1.0
W_BM25 = 1.0
W_MAP = 1.0
W_REGEX = 0.25


# –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (—É—Å–∏–ª–∏–≤–∞–µ–º –≤–∫–ª–∞–¥)
W_CAT = 2.2
CAT_KEYS = ("–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª", "–º–æ–¥–µ—Ä–∞—Ç–æ—Ä", "—ç–∫—Å–ø–µ—Ä—Ç", "–º–∞—Å—Ç–µ—Ä")

# –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–µ –ø–æ–¥–ø—É–Ω–∫—Ç—ã –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–ø.5.x)
CAT_CANON = {
    "–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª": ("5", "4"),
    "–º–æ–¥–µ—Ä–∞—Ç–æ—Ä":    ("5", "1"),
    "—ç–∫—Å–ø–µ—Ä—Ç":      ("5", "3"),
    "–º–∞—Å—Ç–µ—Ä":       ("5", "5"),
}
# —Å–∏–Ω–æ–Ω–∏–º—ã/–≤–∞—Ä–∏–∞–Ω—Ç—ã —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –≤–æ–ø—Ä–æ—Å–∞—Ö (–ª–æ–≤–∏–º –æ–ø–µ—á–∞—Ç–∫–∏ –∏ –∞–Ω–≥–ª/–∫–∑)
CATEGORY_SYNONYMS = {
    "–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª": (
        "–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª", "–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å", "–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç", "–∏—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª",  # –æ–ø–µ—á–∞—Ç–∫–∏/–∫–æ—Ä–Ω–∏
        "research", "issled", "issledovatel", "–∑–µ—Ä—Ç—Ç–µ—É—à", "–∑–µ—Ä—Ç—Ç–µ—É—à—ñ", "pedagog-issled"
    ),
    "–º–æ–¥–µ—Ä–∞—Ç–æ—Ä":    ("–º–æ–¥–µ—Ä–∞—Ç–æ—Ä", "moderator", "moderat"),
    "—ç–∫—Å–ø–µ—Ä—Ç":      ("—ç–∫—Å–ø–µ—Ä—Ç", "expert", "ekspert"),
    "–º–∞—Å—Ç–µ—Ä":       ("–º–∞—Å—Ç–µ—Ä", "master"),
}

# —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–µ –ø–æ–¥–ø–∏—Å–∏ –¥–ª—è –≤—ã–≤–æ–¥–∞
CATEGORY_LABEL = {
    "–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª": "–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å",
    "–º–æ–¥–µ—Ä–∞—Ç–æ—Ä":    "–º–æ–¥–µ—Ä–∞—Ç–æ—Ä",
    "—ç–∫—Å–ø–µ—Ä—Ç":      "—ç–∫—Å–ø–µ—Ä—Ç",
    "–º–∞—Å—Ç–µ—Ä":       "–º–∞—Å—Ç–µ—Ä",
}

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Intent + Policy (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Å–ª–æ–π) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ



INTENT_KEYWORDS = {
    "threshold": (
        "–ø–æ—Ä–æ–≥", "–ø–æ—Ä–æ–≥–æ–≤", "–ø–æ—Ä–æ–≥–æ–≤—ã",
        "–±–∞–ª–ª", "–±–∞–ª–ª—ã", "—Å–∫–æ–ª—å–∫–æ –±–∞–ª–ª–æ–≤",
        "–æ–∑–ø", "–æ—Ü–µ–Ω–∫–∞ –∑–Ω–∞–Ω–∏–π", "—Ç–µ—Å—Ç", "—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω",
        "–ø—Ä–æ—Ü–µ–Ω—Ç", "80%", "–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç", "–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–ª",
        "—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤", "—Å–∫–æ–ª—å–∫–æ –±–∞–ª–ª–æ–≤", "–Ω–∞–¥–æ –Ω–∞–±—Ä–∞—Ç—å"
    ),
    "fee": ("–ø–ª–∞—Ç–∏—Ç—å", "–æ–ø–ª–∞—Ç", "—Å—Ç–æ–∏–º–æ—Å—Ç", "–ø–ª–∞—Ç–Ω–æ", "–±–µ—Å–ø–ª–∞—Ç–Ω", "—Å–±–æ—Ä", "–≥–æ—Å–ø–æ—à–ª–∏–Ω", "–æ–ø–ª–∞—Ç–∞"),
    "periodicity": ("–∫–∞–∫ —á–∞—Å—Ç–æ", "–ø–µ—Ä–∏–æ–¥–∏—á", "–∫–∞–∂–¥—ã–µ –ø—è—Ç—å –ª–µ—Ç", "—Ä–∞–∑ –≤ –ø—è—Ç—å –ª–µ—Ç", "1 —Ä–∞–∑ –≤ 5 –ª–µ—Ç", "–æ–¥–∏–Ω —Ä–∞–∑ –≤ —Ç—Ä–∏ –≥–æ–¥–∞", "1 —Ä–∞–∑ –≤ 3 –≥–æ–¥–∞", "—á–∞—Å—Ç–æ—Ç–∞"),
    "commission": ("–∫—Ç–æ –≤—Ö–æ–¥–∏—Ç", "–∫—Ç–æ –≤—Ö–æ–¥–∏—Ç—å", "—Å–æ—Å—Ç–∞–≤ –∫–æ–º–∏—Å", "—á–ª–µ–Ω—ã –∫–æ–º–∏—Å", "–∫–æ–º–∏—Å—Å–∏—è –ø–æ –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏", "–∫—Ç–æ –≤ –∫–æ–º–∏—Å—Å–∏"),
    # –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å:
    "publications": ("–ø—É–±–ª–∏–∫–∞—Ü", "–∂—É—Ä–Ω–∞–ª", "—Å—Ç–∞—Ç", "scopus", "web of science","wos", "doi", "–∏–Ω–¥–µ–∫—Å–∏—Ä", "—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω"),



    "procedure": ("–∫–∞–∫ —Å–¥–∞—Ç—å", "–∫–∞–∫ –ø—Ä–æ—Ö–æ–¥–∏—Ç", "—ç—Ç–∞–ø", "—ç—Ç–∞–ø—ã", "–∑–∞—è–≤–ª–µ–Ω", "–ø–æ–¥–∞—Ç—å", "–ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ", "–∫–æ–º–∏—Å—Å–∏", "–æ–±–æ–±—â–µ–Ω"),
    "exemption_foreign": (
        "–±–æ–ª–∞—à", "–±–æ–ª–∞—à–∞", "–±–æ–ª–∞—à–∞“õ", "bolash",
        "nazarbayev", "nazarbayev university",
        "–ø–µ—Ä–µ—á–µ–Ω—å —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã—Ö", "–ø–µ—Ä–µ—á–µ–Ω—å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π",
        "–∑–∞—Ä—É–±–µ–∂", "–∑–∞ –≥—Ä–∞–Ω–∏—Ü", "–∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ —Ä–µ—Å–ø—É–±–ª–∏–∫–∏ –∫–∞–∑–∞—Ö—Å—Ç–∞–Ω",
        "–∏–Ω–æ—Å—Ç—Ä–∞–Ω",
        "–º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä", "–¥–æ–∫—Ç–æ—Ä–∞–Ω—Ç—É—Ä", "phd",
        "–∫–∞–Ω–¥–∏–¥–∞—Ç –Ω–∞—É–∫", "–¥–æ–∫—Ç–æ—Ä –Ω–∞—É–∫", "—É—á–µ–Ω–∞—è —Å—Ç–µ–ø–µ–Ω—å", "—É—á—ë–Ω–∞—è —Å—Ç–µ–ø–µ–Ω—å"
    ),
    # –≤–æ–∑–ª–µ INTENT_KEYWORDS["exemption_retirement"]
    "exemption_retirement": (
    "–ø–µ–Ω—Å–∏–æ–Ω–µ—Ä", "—Ä–∞–±–æ—Ç–∞—é—â–∏–π –ø–µ–Ω—Å–∏–æ–Ω–µ—Ä", "–ø–µ–Ω—Å–∏–æ–Ω–Ω–æ–≥–æ –≤–æ–∑—Ä–∞—Å—Ç–∞",
    "–¥–æ –ø–µ–Ω—Å–∏–∏", "–æ—Å—Ç–∞–ª–æ—Å—å –¥–æ –ø–µ–Ω—Å–∏–∏", "–≤–æ–∑—Ä–∞—Å—Ç"
    ),


    # –≤ classify_question, —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ exemption_retirement, –≤—Å—Ç–∞–≤—å —Å–ø–µ—Ü-–ø—Ä–∞–≤–∏–ª–æ:
    if ("–≤–æ–∑—Ä–∞—Å—Ç" in ql or re.search(r"\b\d+\s*(–≥–æ–¥|–ª–µ—Ç|–≥–æ–¥–∞)\b", ql)) and "–∞—Ç—Ç–µ—Å—Ç" in ql:
        return {"intent": "exemption_retirement", "category": None, "confidence": 0.9}


def _detect_category_key(q: str) -> Optional[str]:
    ql = (q or "").lower().replace("—ë","–µ")
    for key, syns in CATEGORY_SYNONYMS.items():
        if any(s in ql for s in syns):
            return key
    return None

def classify_question(q: str) -> Dict[str, Any]:
    ql = (q or "").lower().replace("—ë","–µ")
    cat = _detect_category_key(ql)

    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –ø–µ–Ω—Å–∏–æ–Ω–µ—Ä—ã ‚Üí –∑–∞—Ä—É–±–µ–∂/–ª—å–≥–æ—Ç—ã ‚Üí –æ–ø–ª–∞—Ç–∞ ‚Üí –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å ‚Üí –∫–æ–º–∏—Å—Å–∏—è ‚Üí –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ ‚Üí –ø–æ—Ä–æ–≥ ‚Üí –∫–∞—Ç–µ–≥–æ—Ä–∏—è ‚Üí –ø—Ä–æ—Ü–µ–¥—É—Ä–∞ ‚Üí general
    if any(k in ql for k in INTENT_KEYWORDS["exemption_retirement"]):
        return {"intent": "exemption_retirement", "category": None, "confidence": 0.9}
    # üîΩ –î–û–ë–ê–í–ò–¢–¨ –í–û–¢ –≠–¢–û –°–ü–ï–¶-–ü–†–ê–í–ò–õ–û
    if re.search(r"\b\d+\s*(?:–≥–æ–¥|–ª–µ—Ç|–≥–æ–¥–∞)\b", ql) and "–∞—Ç—Ç–µ—Å—Ç" in ql:
        return {"intent": "exemption_retirement", "category": None, "confidence": 0.9}
    # üîº

    if any(k in ql for k in INTENT_KEYWORDS["exemption_foreign"]):
        return {"intent": "exemption_foreign", "category": None, "confidence": 0.9}

    if any(k in ql for k in INTENT_KEYWORDS["fee"]):
        return {"intent": "fee", "category": None, "confidence": 0.9}

    if any(k in ql for k in INTENT_KEYWORDS["periodicity"]):
        return {"intent": "periodicity", "category": None, "confidence": 0.85}

    if any(k in ql for k in INTENT_KEYWORDS["commission"]):
        return {"intent": "commission", "category": None, "confidence": 0.85}

    if any(k in ql for k in INTENT_KEYWORDS["publications"]):
        return {"intent": "publications", "category": None, "confidence": 0.8}

    if any(k in ql for k in INTENT_KEYWORDS["threshold"]):
        return {"intent": "threshold", "category": cat, "confidence": 0.9}

    if cat:
        return {"intent": "category_requirements", "category": cat, "confidence": 0.85}

    if any(k in ql for k in INTENT_KEYWORDS["procedure"]):
        return {"intent": "procedure", "category": None, "confidence": 0.75}

    return {"intent": "general", "category": None, "confidence": 0.5}


POLICIES = {
    "threshold": {
        "primary": [("39","")],
        "secondary": [("10","")],
        "max_citations": 3,
        "short_template": "–ü–æ –æ–±—â–∏–º –ø—Ä–∞–≤–∏–ª–∞–º: –ø–æ—Ä–æ–≥ –û–ó–ü ‚Äî {threshold_percent} (–ø. 39).{procedure_tail}"
    },
    "category_requirements": {
        "primary": [("5","<cat>")],
        "secondary": [("10",""), ("39","")],
        "max_citations": 3,
        "short_template": "–ü–æ –æ–±—â–∏–º –ø—Ä–∞–≤–∏–ª–∞–º: —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è ¬´–ø–µ–¥–∞–≥–æ–≥–∞-{cat_human}¬ª —Å–º. –ø. 5.{cat_sp}."
    },
    "procedure": {
        "primary": [("10","")],
        "secondary": [],
        "max_citations": 3,
        "short_template": "–ü–æ –æ–±—â–∏–º –ø—Ä–∞–≤–∏–ª–∞–º: —ç—Ç–∞–ø—ã ‚Äî –∑–∞—è–≤–ª–µ–Ω–∏–µ ‚Üí –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ ‚Üí –û–ó–ü ‚Üí –æ–±–æ–±—â–µ–Ω–∏–µ ‚Üí —Ä–µ—à–µ–Ω–∏–µ –∫–æ–º–∏—Å—Å–∏–∏ (–ø.10)."
    },
    "fee": {
        "primary": [("41","")],           # –µ—Å–ª–∏ –µ—Å—Ç—å –≤ –±–∞–∑–µ ‚Äî –ø–æ–ø–∞–¥—ë—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        "secondary": [],
        "max_citations": 2,
        "short_template": "–ü–æ –æ–±—â–∏–º –ø—Ä–∞–≤–∏–ª–∞–º: –≤–æ–ø—Ä–æ—Å—ã –æ–ø–ª–∞—Ç—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –ü—Ä–∞–≤–∏–ª–∞–º–∏; —Å–º. —Ü–∏—Ç–∞—Ç—ã –Ω–∏–∂–µ."
    },
    "periodicity": {
        "primary": [],                    # –ø–æ–¥–±–µ—Ä—ë–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        "secondary": [],
        "max_citations": 2,
        "short_template": "–ü–æ –æ–±—â–∏–º –ø—Ä–∞–≤–∏–ª–∞–º: –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ü—Ä–∞–≤–∏–ª–∞–º–∏; —Å–º. —Ü–∏—Ç–∞—Ç—ã –Ω–∏–∂–µ."
    },
    "commission": {
        "primary": [],                 # –±—ã–ª–æ [("63","")]
        "secondary": [],
        "max_citations": 2,
        "short_template": "–°–æ—Å—Ç–∞–≤ –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–æ–Ω–Ω–æ–π –∫–æ–º–∏—Å—Å–∏–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ü—Ä–∞–≤–∏–ª–∞–º–∏; —Å–º. —Ü–∏—Ç–∞—Ç—ã –Ω–∏–∂–µ."
    },

    "publications": {
        "primary": [],                    # –≤—ã—Ç—è–≥–∏–≤–∞–µ–º –∫—Ä–∏—Ç–µ—Ä–∏–∏ –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ –ø–æ –∫–ª—é—á–∞–º
        "secondary": [("10","")],
        "max_citations": 2,
        "short_template": "–ü—É–±–ª–∏–∫–∞—Ü–∏–∏ —É—á–∏—Ç—ã–≤–∞—é—Ç—Å—è –≤ –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ –ø–æ –ü—Ä–∞–≤–∏–ª–∞–º; —Å–º. –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –≤ —Ü–∏—Ç–∞—Ç–∞—Ö."
    },
    "exemption_foreign": {
        "primary":   [("32","")],
        "secondary": [("10","")],
        "max_citations": 2,
        "short_template": (
            "–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å —É—á—ë–Ω–∞—è —Å—Ç–µ–ø–µ–Ω—å (–∫–∞–Ω–¥./–¥-—Ä –Ω–∞—É–∫/PhD) ‚Äî –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç—Å—è ¬´–ø–µ–¥–∞–≥–æ–≥-–º–æ–¥–µ—Ä–∞—Ç–æ—Ä¬ª –±–µ–∑ –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏ (–ø.32); "
            "–µ—Å–ª–∏ —Å—Ç–µ–ø–µ–Ω–∏ –Ω–µ—Ç ‚Äî –ø—Ä–æ—Ö–æ–¥–∏—Ç–µ –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏—é –ø–æ –æ–±—â–∏–º –ø—Ä–∞–≤–∏–ª–∞–º."
        )
    },
    "exemption_retirement": {
        "primary":   [("30","")],
        "secondary": [("57","")],
        "max_citations": 2,
        "short_template": (
            "–ó–∞–≤–∏—Å–∏—Ç: –µ—Å–ª–∏ –¥–æ –ø–µ–Ω—Å–∏–∏ ‚â§ 4 –ª–µ—Ç ‚Äî –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –æ—Ç –ø—Ä–æ—Ü–µ–¥—É—Ä—ã (–ø.30); "
            "–µ—Å–ª–∏ —É–∂–µ –ø–µ–Ω—Å–∏–æ–Ω–µ—Ä –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç–µ —Ä–∞–±–æ—Ç–∞—Ç—å ‚Äî –¥–µ–π—Å—Ç–≤—É–µ—Ç –æ—Å–æ–±—ã–π –ø–æ—Ä—è–¥–æ–∫ (–ø.57)."
        ),
        "long_template": (
            "–ï—Å–ª–∏ –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø–µ–Ω—Å–∏–æ–Ω–Ω–æ–≥–æ –≤–æ–∑—Ä–∞—Å—Ç–∞ –æ—Å—Ç–∞–ª–æ—Å—å ‚â§ 4 –ª–µ—Ç, –≤—ã –æ—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç–µ—Å—å –æ—Ç –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏; "
            "–∏–º–µ—é—â–∞—è—Å—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –¥–æ –Ω–∞—Å—Ç—É–ø–ª–µ–Ω–∏—è –ø–µ–Ω—Å–∏–∏ (–ø.30). "
            "–ï—Å–ª–∏ –≤—ã —É–∂–µ –¥–æ—Å—Ç–∏–≥–ª–∏ –ø–µ–Ω—Å–∏–æ–Ω–Ω–æ–≥–æ –≤–æ–∑—Ä–∞—Å—Ç–∞ –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç–µ —Ä–∞–±–æ—Ç–∞—Ç—å, –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –ø–æ—Ä—è–¥–æ–∫ –ø.57 "
            "(–∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –æ—Ç –û–ó–ü –∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ–±–æ–±—â–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)."
        )
    },
    "general": {
        "primary": [],
        "secondary": [],
        "max_citations": 3,
        "short_template": "{fallback_short}"
    },
}
def build_short_answer(policy: dict | None, ctx: dict, fallback_short: str) -> str:
    """
    –ï—Å–ª–∏ —É –ø–æ–ª–∏—Ç–∏–∫–∏ –µ—Å—Ç—å —Å–≤–æ–π short_template ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –µ–≥–æ.
    fallback_short –ø—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ –∏–ª–∏ —à–∞–±–ª–æ–Ω–∞.
    """
    if not policy:
        return fallback_short

    tmpl = policy.get("short_template")
    if tmpl:
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, –¥–∞–∂–µ –µ—Å–ª–∏ –∫–∞–∫–∏—Ö-—Ç–æ –∫–ª—é—á–µ–π –Ω–µ—Ç –≤ ctx
        try:
            return tmpl.format(**(ctx or {}))
        except Exception:
            return tmpl  # –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å, —á—Ç–æ–±—ã –Ω–µ —É–ø–∞—Å—Ç—å

    return fallback_short

def _policy_primary_pairs(intent: str, category_key: Optional[str]) -> List[Tuple[str,str]]:
    pairs = []
    if intent not in POLICIES: return pairs
    for pn, sp in POLICIES[intent]["primary"]:
        if sp == "<cat>":
            if not category_key: continue
            pn_c, sp_c = CAT_CANON.get(category_key, ("",""))
            if pn_c: pairs.append((pn_c, sp_c))
        else:
            pairs.append((pn, sp))
    return pairs

def _human_cat(category_key: Optional[str]) -> Tuple[str, str]:
    if not category_key: return ("", "")
    human = CATEGORY_LABEL.get(category_key, category_key)
    _, sp = CAT_CANON.get(category_key, ("",""))
    return (human, sp)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Slot-extractors (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª–∏ —Ñ–∞–∫—Ç–æ–≤) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def build_procedure_tail_if_p10(punkts: List[Dict[str,Any]]) -> str:
    for p in punkts:
        if str(p.get("punkt_num","")).strip() == "10":
            return " (—ç—Ç–∞–ø—ã: –∑–∞—è–≤–ª–µ–Ω–∏–µ ‚Üí –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ ‚Üí –û–ó–ü ‚Üí –æ–±–æ–±—â–µ–Ω–∏–µ ‚Üí —Ä–µ—à–µ–Ω–∏–µ –∫–æ–º–∏—Å—Å–∏–∏)"
    return ""
def extract_threshold_percent_from_p39_for_category(punkts: List[Dict[str,Any]], category_key: Optional[str]) -> Optional[str]:
    """
    –ü—ã—Ç–∞–µ–º—Å—è –≤—ã—Ç–∞—â–∏—Ç—å % –∏–∑ –ø.39 –∏–º–µ–Ω–Ω–æ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–º–∞—Å—Ç–µ—Ä/—ç–∫—Å–ø–µ—Ä—Ç/‚Ä¶).
    –õ–æ–≥–∏–∫–∞:
      1) –ù–∞—Ö–æ–¥–∏–º –ø.39.
      2) –ï—Å–ª–∏ –∑–∞–¥–∞–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è, –∏—â–µ–º –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π/—Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, –≥–¥–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –∫–æ—Ä–µ–Ω—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.
      3) –ë–µ—Ä—ë–º –±–ª–∏–∂–∞–π—à–∏–π % –∫ —Ç–∞–∫–∏–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º.
      4) –§–æ–ª–ª–±–µ–∫ ‚Äî None (–ø—É—Å—Ç—å —Å—Ä–∞–±–æ—Ç–∞–µ—Ç –æ–±—â–∏–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä/–¥–µ—Ñ–æ–ª—Ç).
    """
    if not category_key:
        return None

    target_syns = CATEGORY_SYNONYMS.get(category_key, ())
    if not target_syns:
        return None

    text39 = None
    for p in punkts:
        if str(p.get("punkt_num","")).strip() == "39":
            text39 = (p.get("text") or "")
            break
    if not text39:
        return None

    tl = text39.lower().replace("—ë","–µ")

    # –†–µ–∂–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –∫—Ä—É–ø–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
    parts = re.split(r"(?:\n+|[.;]\s+)", tl)
    # —Å–æ–±–∏—Ä–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç—ã, –≥–¥–µ –µ—Å—Ç—å —Å–∏–Ω–æ–Ω–∏–º—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    cand = [s for s in parts if any(syn in s for syn in target_syns)]
    if not cand:
        # –∏–Ω–æ–≥–¥–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è —É–ø–æ–º—è–Ω—É—Ç–∞ —Ç–æ–ª—å–∫–æ –∫–∞–∫ ¬´–ø–µ–¥–∞–≥–æ–≥-–º–∞—Å—Ç–µ—Ä¬ª
        base = tl
        cand = [base] if any(syn in base for syn in target_syns) else []

    # –¥–æ—Å—Ç–∞—ë–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã –∏–∑ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    get_perc = re.compile(r"(\d{1,3})\s*%")
    found: List[int] = []
    for s in cand:
        found += [int(x) for x in get_perc.findall(s)]
    if found:
        # –±–µ—Ä—ë–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∏–∑ –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã—Ö –∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ‚Äî –±–µ–∑–æ–ø–∞—Å–Ω–µ–µ, –µ—Å–ª–∏ –≤ –æ–¥–Ω–æ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∏—Å–µ–ª
        return f"{max(found)}%"

    return None

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Policy-aware helpers –¥–ª—è —Ü–∏—Ç–∞—Ç –∏ –∫—Ä–∞—Ç–∫–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def ensure_min_citations_policy(question: str,
                                data: Dict[str,Any],
                                punkts: List[Dict[str,Any]],
                                intent_info: Dict[str,Any]) -> Dict[str,Any]:
    intent = intent_info.get("intent","general")
    category_key = intent_info.get("category")
    primary = _policy_primary_pairs(intent, category_key)
    # —Ñ–æ—Ä–º–∏—Ä—É–µ–º must-have —Å–ø–∏—Å–æ–∫: primary –∏–∑ –ø–æ–ª–∏—Ç–∏–∫–∏ + –µ—Å–ª–∏ –µ—Å—Ç—å ‚Äî –≤—Ç–æ—Ä–∏—á–Ω—ã–µ –∏–∑ –ø–æ–ª–∏—Ç–∏–∫–∏
    want = list(primary)
    for pn, sp in POLICIES.get(intent, {}).get("secondary", []):
        if sp == "<cat>":
            if category_key:
                pn_c, sp_c = CAT_CANON.get(category_key, ("",""))
                if pn_c: want.append((pn_c, sp_c))
        else:
            want.append((pn, sp))

    # –æ—Å—Ç–∞–≤–∏–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ü–∏—Ç–∞—Ç—ã, –Ω–æ –ø–µ—Ä–µ–¥ –Ω–∏–º–∏ –≤—Å—Ç–∞–≤–∏–º –Ω—É–∂–Ω—ã–µ –ø—É–Ω–∫—Ç—ã (–µ—Å–ª–∏ –æ–Ω–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ)
    have = {(str(c.get("punkt_num","")).strip(), str(c.get("subpunkt_num","")).strip())
            for c in (data.get("citations") or [])}
    out: List[Dict[str,str]] = []

    def _exists(pn: str, sp: str="") -> bool:
        for p in punkts:
            if str(p.get("punkt_num","")).strip()==pn and (sp=="" or str(p.get("subpunkt_num","")).strip()==sp):
                return True
        return False

    for pn, sp in want:
        if _exists(pn, sp) and (pn, sp) not in have:
            out.append({"punkt_num": pn, "subpunkt_num": sp, "quote": ""})

    # –¥–æ–±–∞–≤–∏–º –Ω–∞–∑–∞–¥ —Å—Ç–∞—Ä—ã–µ (–±–µ–∑ –¥—É–±–ª–µ–π)
    for c in (data.get("citations") or []):
        pn = str(c.get("punkt_num","")).strip()
        sp = str(c.get("subpunkt_num","")).strip()
        if (pn, sp) not in {(x["punkt_num"], x["subpunkt_num"]) for x in out}:
            out.append({"punkt_num": pn, "subpunkt_num": sp, "quote": c.get("quote","")})

    # –æ–≥—Ä–∞–Ω–∏—á–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º
    maxc = POLICIES.get(intent, {}).get("max_citations", 3)
    data["citations"] = out[:maxc]
    return data

def enforce_short_answer_policy(question: str,
                                data: Dict[str,Any],
                                punkts: List[Dict[str,Any]],
                                intent_info: Dict[str,Any]) -> Dict[str,Any]:
    intent = intent_info.get("intent","general")
    category_key = intent_info.get("category")
    human, sp = _human_cat(category_key)

    def _is_category_q() -> bool:
        return bool(category_key)

    # –±–∞–∑–æ–≤—ã–π –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç –∏–∑ –ø–æ–ª–∏—Ç–∏–∫–∏
    policy = POLICIES.get(intent, POLICIES["general"])
    templ = policy.get("short_template","{fallback_short}")

    facts = {
        "cat_human": human,
        "cat_sp": sp,
        "threshold_percent": (
            extract_threshold_percent_from_p39_for_category(punkts, category_key)
            or extract_threshold_percent_from_p39(punkts)
            or "80%"
        ),
        "procedure_tail": build_procedure_tail_if_p10(punkts)
    }

    # –í–ê–ñ–ù–û: –ø—Ä–æ—Ü–µ–Ω—Ç –∏ ¬´—Ö–≤–æ—Å—Ç —ç—Ç–∞–ø–æ–≤¬ª –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –¢–û–õ–¨–ö–û –¥–ª—è threshold/category
    if intent not in {"threshold", "category_requirements"}:
        facts["threshold_percent"] = ""
        facts["procedure_tail"] = ""

    fallback = (data.get("short_answer") or "–ü–æ –æ–±—â–∏–º –ø—Ä–∞–≤–∏–ª–∞–º.").strip()
    sa = templ.format(fallback_short=fallback, **facts).strip()

    # –î–æ–ø. –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ
    if _is_category_q() and "—Å–º. –ø." not in sa:
        pn, spn = CAT_CANON.get(category_key, ("",""))
        tag = f" (—Å–º. –ø. {pn}.{spn})" if pn and spn else ""
        if tag and len(sa) + len(tag) <= 200:
            sa += tag

    data["short_answer"] = sa[:200]
    return data




def policy_get_must_have_pairs(intent_info: Dict[str,Any]) -> List[Tuple[str,str]]:
    intent = intent_info.get("intent","general")
    category_key = intent_info.get("category")
    pairs = _policy_primary_pairs(intent, category_key)

    # –¥–æ–±–∞–≤–ª—è–µ–º secondary –¥–ª—è –õ–Æ–ë–û–ô –ø–æ–ª–∏—Ç–∏–∫–∏
    for pn, sp in POLICIES.get(intent, {}).get("secondary", []):
        if sp == "<cat>":
            if category_key:
                pn_c, sp_c = CAT_CANON.get(category_key, ("",""))
                if pn_c:
                    pairs.append((pn_c, sp_c))
        else:
            pairs.append((pn, sp))
    return pairs





# –∫–ª—é—á–∏, –ø–æ –∫–æ—Ç–æ—Ä—ã–º —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ —Ä–µ—á—å –ø—Ä–æ –û–ó–ü/–ø–æ—Ä–æ–≥ (–¥–ª—è –ø.39)
KW_OZP_TERMS = ("–æ–∑–ø", "–æ—Ü–µ–Ω–∫–∞ –∑–Ω–∞–Ω–∏–π –ø–µ–¥–∞–≥–æ–≥–æ–≤", "–ø–æ—Ä–æ–≥", "—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω", "80 %", "80%")

# –ª–∏–º–∏—Ç—ã –¥–ª–∏–Ω—ã —Ü–∏—Ç–∞—Ç
QUOTE_WIDTH_DEFAULT = int(os.environ.get("QUOTE_WIDTH_DEFAULT", "180"))
QUOTE_WIDTH_LONG    = int(os.environ.get("QUOTE_WIDTH_LONG", "600"))


def kw_category_boost(question: str, doc_text: str) -> float:
    ql = (question or "").lower().replace("—ë", "–µ")
    dl = (doc_text or "").lower().replace("—ë", "–µ")
    # –µ—Å—Ç—å –ª–∏ –≤ –≤–æ–ø—Ä–æ—Å–µ —Ö–æ—Ç—å –æ–¥–∏–Ω —Å–∏–Ω–æ–Ω–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    trig = None
    for key, syns in CATEGORY_SYNONYMS.items():
        if any(s in ql for s in syns):
            trig = key
            break
    if not trig:
        return 0.0
    variants = ("–ø–µ–¥–∞–≥–æ–≥-", "–ø–µ–¥–∞–≥–æ–≥ ‚Äî", "–ø–µ–¥–∞–≥–æ–≥ ‚Äì", "–ø–µ–¥–∞–≥–æ–≥ ")
    # –∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ —è–≤–Ω–æ —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è –Ω—É–∂–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
    if trig in dl or any((v + trig) in dl for v in variants):
        return 1.0
    return 0.0



W_KW = 1.1  # –≤–µ—Å —Å–∏–≥–Ω–∞–ª–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –¥–ª—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π/–∑–∞—Ä—É–±–µ–∂
KW_EXCEPTION_TERMS = (
    "–±–µ–∑ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏",
    "–ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç—Å—è –±–µ–∑",
    "–æ—Å–≤–æ–±–æ–∂–¥–∞",
    "–Ω–µ –ø–æ–¥–ª–µ–∂–∏—Ç –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏",
)
KW_FOREIGN_TERMS = (
    "–∑–∞—Ä—É–±–µ–∂", "–∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ —Ä–µ—Å–ø—É–±–ª–∏–∫–∏ –∫–∞–∑–∞—Ö—Å—Ç–∞–Ω", "–∑–∞ –≥—Ä–∞–Ω–∏—Ü",
    "–∏–Ω–æ—Å—Ç—Ä–∞–Ω", "nazarbayev university", "–±–æ–ª–∞—à", "–±–æ–ª–∞—à–∞“õ",
)
KW_PERIOD_TERMS = ("–∫–∞–∂–¥—ã–µ –ø—è—Ç—å –ª–µ—Ç", "–Ω–µ —Ä–µ–∂–µ –æ–¥–Ω–æ–≥–æ —Ä–∞–∑–∞", "–ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç", "—Ä–∞–∑ –≤ –ø—è—Ç—å –ª–µ—Ç", "–æ–¥–∏–Ω —Ä–∞–∑ –≤ —Ç—Ä–∏ –≥–æ–¥–∞")
KW_FEE_TERMS = ("–æ–ø–ª–∞—Ç", "–ø–ª–∞—Ç", "—Å—Ç–æ–∏–º–æ—Å—Ç", "–±–µ—Å–ø–ª–∞—Ç", "–≥–æ—Å–ø–æ—à–ª–∏–Ω", "—Å–±–æ—Ä")
KW_COMMISSION_TERMS = ("–∫–æ–º–∏—Å—Å–∏", "—Å–æ—Å—Ç–∞–≤", "—á–ª–µ–Ω—ã –∫–æ–º–∏—Å", "–∫–æ–º–∏—Å—Å–∏—è –ø–æ –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏")
KW_PUBLICATION_TERMS = ("–ø—É–±–ª–∏–∫–∞—Ü", "–∂—É—Ä–Ω–∞–ª", "—Å—Ç–∞—Ç—å", "scopus", "web of science", "wos", "doi", "—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω")

def kw_boost(question: str, doc_text: str) -> float:
    ql = (question or "").lower().replace("—ë", "–µ")
    dl = (doc_text or "").lower().replace("—ë", "–µ")

    boost = 0.0
    is_category_q = any(any(s in ql for s in syns) for syns in CATEGORY_SYNONYMS.values())

    foreign_q = any(k in ql for k in ("–º–∞–≥–∏—Å—Ç", "–∑–∞ —Ä—É–±–µ–∂", "–∑–∞ –≥—Ä–∞–Ω–∏—Ü", "–∑–∞—Ä—É–±–µ–∂", "–∏–Ω–æ—Å—Ç—Ä–∞–Ω", "–±–æ–ª–∞—à", "–±–æ–ª–∞—à–∞“õ"))
    if foreign_q and not is_category_q:
        if any(k in dl for k in KW_EXCEPTION_TERMS):
            boost += 0.6
        if any(k in dl for k in KW_FOREIGN_TERMS):
            boost += 0.6

    has_exception_phrase = any(k in dl for k in KW_EXCEPTION_TERMS)
    if has_exception_phrase:
        boost += 0.3
        if is_category_q:
            boost -= 0.6

    if (is_category_q or foreign_q) and any(k in dl for k in KW_PERIOD_TERMS):
        boost -= 0.4

    if is_category_q and any(t in dl for t in KW_OZP_TERMS):
        boost += 0.5

    # –Ω–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã:
    if any(k in ql for k in ("–ø–ª–∞—Ç–∏—Ç—å","–æ–ø–ª–∞—Ç","—Å—Ç–æ–∏–º–æ—Å—Ç","–±–µ—Å–ø–ª–∞—Ç","—Å–±–æ—Ä","–≥–æ—Å–ø–æ—à–ª–∏–Ω")) and any(k in dl for k in KW_FEE_TERMS):
        boost += 0.7

    if any(k in ql for k in ("–∫—Ç–æ –≤—Ö–æ–¥–∏—Ç","—Å–æ—Å—Ç–∞–≤ –∫–æ–º–∏—Å–∏","—Å–æ—Å—Ç–∞–≤ –∫–æ–º–∏—Å","—á–ª–µ–Ω—ã –∫–æ–º–∏—Å","–∫–æ–º–∏—Å—Å–∏—è")) and any(k in dl for k in KW_COMMISSION_TERMS):
        boost += 0.6

    if any(k in ql for k in ("–ø—É–±–ª–∏–∫–∞—Ü","–∂—É—Ä–Ω–∞–ª","—Å—Ç–∞—Ç—å","scopus","wos","web of science","doi")) and any(k in dl for k in KW_PUBLICATION_TERMS):
        boost += 0.6

    if any(k in ql for k in ("–∫–∞–∫ —á–∞—Å—Ç–æ","–ø–µ—Ä–∏–æ–¥–∏—á","–∫–∞–∂–¥—ã–µ –ø—è—Ç—å –ª–µ—Ç","—Ä–∞–∑ –≤ –ø—è—Ç—å –ª–µ—Ç","–æ–¥–∏–Ω —Ä–∞–∑ –≤ —Ç—Ä–∏ –≥–æ–¥–∞","1 —Ä–∞–∑ –≤ 3 –≥–æ–¥–∞")) and any(k in dl for k in KW_PERIOD_TERMS):
        boost += 0.6

    return boost

   


TELEGRAM_API = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}"

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –õ–æ–≥–≥–µ—Ä ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
)
logger = logging.getLogger("rag-bot")
LAST_RESPONSES: Dict[Tuple[int, int], Dict[str, Any]] = {}
# --- –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–∏–µ –æ–±—ë—Ä—Ç–∫–∏ –¥–ª—è sync-—Ñ—É–Ω–∫—Ü–∏–π ---
async def run_blocking(fn, *args, **kwargs):
    return await asyncio.to_thread(fn, *args, **kwargs)

# --- –ø–µ—Ä-—á–∞—Ç–æ–≤–∞—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ —á–∞—Ç–∞ ---
LOCKS: Dict[int, asyncio.Lock] = {}

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ö–ª–∏–µ–Ω—Ç—ã –∏ –¥–∞–Ω–Ω—ã–µ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

client = OpenAI(api_key=OPENAI_API_KEY)

def load_punkts(path: str) -> List[Dict[str, Any]]:
    import re
    with open(path, "r", encoding="utf-8") as f:
        arr = json.load(f)
    assert isinstance(arr, list) and len(arr) > 0, "PUNKTS JSON must be a non-empty list"
    for p in arr:
        p.setdefault("id", str(uuid.uuid4()))
        p.setdefault("punkt_num", "")
        p.setdefault("subpunkt_num", "")
        p.setdefault("text", "")
        p.setdefault("chapter", "")
        p.setdefault("paragraph", "")
        # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ¬´5.3)¬ª ‚Üí ¬´5¬ª –∏ ¬´3¬ª
        p["punkt_num"] = re.sub(r"\D+", "", str(p["punkt_num"]))
        p["subpunkt_num"] = re.sub(r"\D+", "", str(p["subpunkt_num"]))
    return arr



PUNKTS: List[Dict[str, Any]] = load_punkts(PUNKTS_PATH)


# embeddings.npy ‚Äî memmap –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏/–±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
PUNKT_EMBS: np.ndarray = np.load(EMBEDDINGS_PATH, mmap_mode="r")
assert PUNKT_EMBS.ndim == 2, "embeddings.npy must be 2D"
assert PUNKT_EMBS.shape[0] == len(PUNKTS), "rows(embeddings) != len(PUNKTS)"

EMB_DIM = int(PUNKT_EMBS.shape[1])
if EMB_DIM not in (1536, 3072):
    raise AssertionError(f"Unexpected embedding dimension: {EMB_DIM} (expected 1536 or 3072)")

# –ê–≤—Ç–æ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —Ñ–∞–π–ª–æ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
if EMB_DIM == 1536 and EMBEDDING_MODEL != "text-embedding-ada-002":
    logger.warning("Detected 1536-dim embeddings.npy ‚Üí –ø–µ—Ä–µ–∫–ª—é—á–∞—é EMBEDDING_MODEL –Ω–∞ text-embedding-ada-002")
    EMBEDDING_MODEL = "text-embedding-ada-002"
elif EMB_DIM == 3072 and EMBEDDING_MODEL != "text-embedding-3-large":
    logger.warning("Detected 3072-dim embeddings.npy ‚Üí –ø–µ—Ä–µ–∫–ª—é—á–∞—é EMBEDDING_MODEL –Ω–∞ text-embedding-3-large")
    EMBEDDING_MODEL = "text-embedding-3-large"

logger.info("Loaded %d punkts; embeddings: %s; model=%s", len(PUNKTS), PUNKT_EMBS.shape, EMBEDDING_MODEL)


def tokenize(text: str) -> List[str]:
    return re.findall(r"[–∞-—è—ëa-z0-9]+", (text or "").lower())

def normalize_tokens(tokens: List[str]) -> List[str]:
    if not _MORPH:
        return tokens
    out = []
    for t in tokens:
        try:
            p = _MORPH.parse(t)
            out.append(p[0].normal_form if p else t)
        except Exception:
            out.append(t)
    return out
# –¢–µ–∫—Å—Ç—ã –¥–ª—è BM25/keyword –ø–æ–∏—Å–∫–∞
DOCS_TOKENS: List[List[str]] = []
for p in PUNKTS:
    toks = tokenize(p.get("text") or "")
    DOCS_TOKENS.append(normalize_tokens(toks))


BM25 = None
if HAVE_BM25:
    BM25 = BM25Okapi(DOCS_TOKENS)
    logger.info("BM25 index built (rank_bm25).")
else:
    logger.warning("rank_bm25 not installed ‚Äî will use simple keyword scoring fallback.")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –£—Ç–∏–ª–∏—Ç—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def cosine_sim(a: np.ndarray, b: np.ndarray) -> np.ndarray:
    # a: (D,), b: (N, D)
    a_norm = a / (np.linalg.norm(a) + 1e-12)
    b_norm = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-12)
    return (b_norm @ a_norm)

def normalize_query(q: str) -> str:
    q = q or ""
    q = q.replace("—ë", "–µ")
    return q.strip()




def call_with_retries(fn, max_attempts=3, base_delay=1.0, *args, **kwargs):
    for attempt in range(1, max_attempts + 1):
        try:
            return fn(*args, **kwargs)
        except (RateLimitError, APITimeoutError, APIConnectionError, APIStatusError) as e:
            if attempt == max_attempts:
                raise
            sleep_s = base_delay * (2 ** (attempt - 1)) + (0.1 * attempt)
            logger.warning("OpenAI call failed (%s). Retry %d/%d in %.1fs", type(e).__name__, attempt, max_attempts, sleep_s)
            time.sleep(sleep_s)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def multi_query_rewrites(q: str, n: int = 3) -> List[str]:
    if not MULTI_QUERY:
        return [q]
    prompt = f"–ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π/–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–æ–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º, —Å–æ—Ö—Ä–∞–Ω–∏–≤ —Å–º—ã—Å–ª. –î–∞–π {n} –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤, –ø–æ –æ–¥–Ω–æ–º—É –≤ —Å—Ç—Ä–æ–∫–µ, –±–µ–∑ –Ω—É–º–µ—Ä–∞—Ü–∏–∏ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤:\n\n{q}"
    resp = call_with_retries(
        client.chat.completions.create,
        model=CHAT_MODEL,  # –≤–º–µ—Å—Ç–æ "gpt-4o-mini"
        messages=[
            {"role": "system", "content": "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –ø—Ä–∞–≤–æ–≤—ã–º/–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.2,
    )
    text = resp.choices[0].message.content.strip()
    variants = [v.strip() for v in text.split("\n") if v.strip()]
    if not variants:
        variants = [q]
    # –î–µ–¥—É–ø
    uniq = []
    for v in [q] + variants:
        if v not in uniq:
            uniq.append(v)
    return uniq[: n + 1]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _topk_desc(arr: np.ndarray, k: int) -> np.ndarray:
    k = max(1, min(k, arr.size))
    idx = np.argpartition(-arr, k-1)[:k]
    return idx[np.argsort(-arr[idx])]


def vector_search(q: str, top_k: int = 100) -> List[Tuple[int, float]]:
    vec = embed_query(q)
    b = PUNKT_EMBS
    a_norm = vec / (np.linalg.norm(vec) + 1e-12)
    b_norm = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-12)
    sims = (b_norm @ a_norm)
    idxs = _topk_desc(sims, top_k)
    return [(int(i), float(sims[i])) for i in idxs]

def bm25_search(q: str, top_k: int = 100) -> List[Tuple[int, float]]:
    toks = normalize_tokens(tokenize(q))
    if BM25:
        scores = np.array(BM25.get_scores(toks), dtype=np.float64)
    else:
        # –ø—Ä–æ—Å—Ç–æ–π TF fallback
        scores_list = []
        for doc in DOCS_TOKENS:
            if not doc:
                scores_list.append(0.0); continue
            s = 0
            for t in toks:
                s += doc.count(t)
            scores_list.append(float(s))
        scores = np.array(scores_list, dtype=np.float64)
    idxs = _topk_desc(scores, top_k)
    return [(int(i), float(scores[i])) for i in idxs]


def hyde_passage(question: str) -> Optional[str]:
    if not HYDE:
        return None
    prompt = (
        "–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –∫—Ä–∞—Ç–∫–∏–π –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç (5‚Äì7 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π) –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ –ü—Ä–∞–≤–∏–ª–∞–º, "
        "–±–µ–∑ –≤—ã–¥—É–º–∫–∏ —Ñ–∞–∫—Ç–æ–≤, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –æ–±—â–∏–π. –≠—Ç–æ —á–µ—Ä–Ω–æ–≤–æ–π –∫–æ–Ω—Å–ø–µ–∫—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞, –Ω–µ –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç.\n\n"
        f"–í–æ–ø—Ä–æ—Å: {question}"
    )
    try:
        resp = call_with_retries(
            client.chat.completions.create,
            model=CHAT_MODEL,
            messages=[
                {"role": "system", "content": "–¢—ã –∞–∫–∫—É—Ä–∞—Ç–Ω–æ —Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—à—å —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Å–ø–µ–∫—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.2,
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception:
        return None


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ú–∞–ø–ø–∏–Ω–≥–∏ –∏ —Ä–µ–≥—ç–∫—Å–ø—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

KEY_REGEXES = [
    r"\b–ø\.?\s*(\d{1,3})(?:\.(\d{1,3}))?\b",
    r"\b–ø—É–Ω–∫—Ç[–∞-—è]*\s*(\d{1,3})(?:\.(\d{1,3}))?\b",
    r"\b–ø–æ–¥–ø—É–Ω–∫—Ç[–∞-—è]*\s*(\d{1,3})\.(\d{1,3})\b",
    r"\b–ø–ø\.\s*(\d{1,3})\.(\d{1,3})\b",
]


def regex_hits(q: str) -> List[int]:
    hits: List[int] = []
    ql = (q or "").lower()
    for rgx in KEY_REGEXES:
        for m in re.finditer(rgx, ql):
            pn = (m.group(1) or "").strip()
            sp = (m.group(2) or "").strip() if (m.lastindex or 0) >= 2 else ""
            for i, p in enumerate(PUNKTS):
                if str(p.get("punkt_num","")).strip() == pn and (not sp or str(p.get("subpunkt_num","")).strip() == sp):
                    hits.append(i)
    # —É–±—Ä–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω–∏–≤ –ø–æ—Ä—è–¥–æ–∫
    return list(dict.fromkeys(hits))


def mapped_hits(q: str) -> List[int]:
    ql = q.lower()
    if not UNIVERSAL_MAP:
        return []
    got: List[int] = []
    for key, coord in UNIVERSAL_MAP.items():
        if key in ql:
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Ç–∞–∫–∏–º –ø—É–Ω–∫—Ç–æ–º; –µ—Å–ª–∏ subpunkt_num –∑–∞–¥–∞–Ω, —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –Ω–µ–º—É
            for i, p in enumerate(PUNKTS):
                if p.get("punkt_num") == coord.get("punkt_num", ""):
                    sp = coord.get("subpunkt_num", "")
                    if not sp or p.get("subpunkt_num") == sp:
                        got.append(i)
    return list(dict.fromkeys(got))

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Merge & Score ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


# ---- –Ω–µ–±–æ–ª—å—à–æ–π LRU-–∫—ç—à –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ ----
_EMB_CACHE: "OrderedDict[str, np.ndarray]" = OrderedDict()
_EMB_CACHE_CAP = 512
_EMB_LOCK = threading.Lock()  # NEW

def _emb_cache_get(key: str) -> Optional[np.ndarray]:
    with _EMB_LOCK:
        if key in _EMB_CACHE:
            vec = _EMB_CACHE.pop(key)
            _EMB_CACHE[key] = vec
            return vec
    return None

def _emb_cache_put(key: str, vec: np.ndarray) -> None:
    with _EMB_LOCK:
        if key in _EMB_CACHE:
            _EMB_CACHE.pop(key)
        _EMB_CACHE[key] = vec
        if len(_EMB_CACHE) > _EMB_CACHE_CAP:
            _EMB_CACHE.popitem(last=False)


def embed_query(text: str) -> np.ndarray:  # override
    key = (text or "").strip()
    got = _emb_cache_get(key)
    if got is not None:
        return got
    resp = call_with_retries(
        client.embeddings.create,
        model=EMBEDDING_MODEL,
        input=key,
    )
    vec = np.array(resp.data[0].embedding, dtype=np.float64)
    _emb_cache_put(key, vec)
    return vec

# ---- rag_search —Å —É—Å–ª–æ–≤–Ω—ã–º HyDE –∏ –±–æ–ª–µ–µ —É–∑–∫–∏–º final_k –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π ----
def rag_search(q: str, top_k_stage1: int = 120, final_k: int = 45,
               must_have_pairs: Optional[List[Tuple[str,str]]] = None) -> List[Dict[str, Any]]:
    q = normalize_query(q)
    ql = q.lower().replace("—ë", "–µ")

    def _is_cat_q(ql_: str) -> Optional[str]:
        for key, syns in CATEGORY_SYNONYMS.items():
            if any(s in ql_ for s in syns):
                return key
        return None

    cat_key = _is_cat_q(ql)
    is_category_q = cat_key is not None
    if is_category_q:
        final_k = min(final_k, 24)  # –±—ã–ª–æ 30, —Å—É–∑–∏–º –µ—â—ë —Å–∏–ª—å–Ω–µ–µ

    variants = multi_query_rewrites(q)
    dense_agg: Dict[int, float] = {}
    sparse_agg: Dict[int, float] = {}
    # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Å—É–∂–µ–Ω–∏–µ –≤—ã–¥–∞—á–∏ –ø–æ –∏–Ω—Ç–µ–Ω—Ç—É
    try:
        info_int = classify_question(q)
        if info_int and info_int.get("intent") in {"commission", "fee", "publications"}:
            final_k = min(final_k, 20)
    except Exception:
        pass

    # Dense pass 1
    for v in variants:
        for idx, sc in vector_search(v, top_k=top_k_stage1):
            dense_agg[idx] = max(dense_agg.get(idx, 0.0), sc)

    # –£—Å–ª–æ–≤–Ω—ã–π HyDE: –≤–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–∏–≥–Ω–∞–ª —Å–ª–∞–±—ã–π
    best_dense = (max(dense_agg.values()) if dense_agg else 0.0)
    if HYDE and best_dense < 0.30:
        hyde = hyde_passage(q)
        if hyde:
            for idx, sc in vector_search(hyde, top_k=top_k_stage1 // 2):
                dense_agg[idx] = max(dense_agg.get(idx, 0.0), sc)

    # Sparse
    for v in [q] + variants:
        for idx, sc in bm25_search(v, top_k=top_k_stage1):
            sparse_agg[idx] = max(sparse_agg.get(idx, 0.0), sc)

    # Regex/Mapping
    regex_idx = regex_hits(q)
    mapped_idx = mapped_hits(q)

    # z-score –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è sparse
    if sparse_agg:
        vals = np.array(list(sparse_agg.values()), dtype=np.float64)
        mu = float(vals.mean()); sigma = float(vals.std() + 1e-6)
        for k in list(sparse_agg.keys()):
            sparse_agg[k] = (sparse_agg[k] - mu) / sigma

    # —Å–≤–æ–¥–Ω—ã–π —Å–∫–æ—Ä
    items: List[Tuple[int, float]] = []
    candidate_ids = set(list(dense_agg.keys()) + list(sparse_agg.keys()) + regex_idx + mapped_idx)
    for idx in candidate_ids:
        txt = PUNKTS[idx].get("text", "")
        total = (
            W_EMB * dense_agg.get(idx, 0.0)
            + W_BM25 * sparse_agg.get(idx, 0.0)
            + W_REGEX * (1.0 if idx in regex_idx else 0.0)
            + W_MAP * (1.0 if idx in mapped_idx else 0.0)
            + W_KW  * kw_boost(q, txt)
            + W_CAT * kw_category_boost(q, txt)
        )
        items.append((idx, total))

    items.sort(key=lambda x: -x[1])
    ranked = [i for i, _ in items]

    # must-have
    must_have: List[int] = []
    must_have.extend(mapped_idx + regex_idx)

    if is_category_q:
        for i, p in enumerate(PUNKTS):
            txt = (p.get("text") or "").lower().replace("—ë", "–µ")
            if cat_key and cat_key in txt:
                must_have.append(i)
        pn, sp = CAT_CANON.get(cat_key, ("", ""))
        if pn:
            canon_ids = [i for i, p in enumerate(PUNKTS)
                         if str(p.get("punkt_num","")).strip()==pn
                         and str(p.get("subpunkt_num","")).strip()==sp]
            must_have = canon_ids + must_have
        for wanted in ("10", "39"):
            add = [i for i, p in enumerate(PUNKTS) if str(p.get("punkt_num","")).strip()==wanted]
            must_have.extend(add)

    


  # ‚îÄ‚îÄ –î–û–ë–ê–í–ò–¢–¨: –≤–Ω–µ—à–Ω–∏–µ must-have –ø–æ –ø–æ–ª–∏—Ç–∏–∫–µ ‚îÄ‚îÄ
    if must_have_pairs:
        for pn, sp in must_have_pairs:
            for i, p in enumerate(PUNKTS):
                if str(p.get("punkt_num","")).strip() == pn and (not sp or str(p.get("subpunkt_num","")).strip() == sp):
                    # –≤—Å—Ç–∞–≤–∏–º –≤ –Ω–∞—á–∞–ª–æ must_have, –µ—Å–ª–∏ —Ç–∞–∫–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –µ—â—ë –Ω–µ—Ç
                    if i not in must_have:
                        must_have.insert(0, i)
    top_idx: List[int] = []
    for i in must_have + ranked:
        if i not in top_idx:
            top_idx.append(i)
        if len(top_idx) >= final_k:
            break

    return [PUNKTS[i] for i in top_idx]
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (LLM) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

SYSTEM_PROMPT = (
    "–¢—ã ‚Äî —Å—Ç—Ä–æ–≥–∏–π –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –ü—Ä–∞–≤–∏–ª–∞–º –∏ —É—Å–ª–æ–≤–∏—è–º –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏ –ø–µ–¥–∞–≥–æ–≥–æ–≤ –†–ö. "
    "–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –ø–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º –ø—É–Ω–∫—Ç–∞–º, –Ω–µ –≤—ã–¥—É–º—ã–≤–∞–π –¥–µ—Ç–∞–ª–µ–π. "
    "–ï—Å–ª–∏ –ø—Ä—è–º–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç, —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏. –í—Å–µ–≥–¥–∞ —Å—Å—ã–ª–∞–π—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—É–Ω–∫—Ç—ã."
)

GEN_PROMPT_TEMPLATE = """\
–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
{question}

–ö–æ–Ω—Ç–µ–∫—Å—Ç (—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –ü—Ä–∞–≤–∏–ª):
{context}

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ç–≤–µ—Ç—É (–°–¢–†–û–ì–û):
1) –í–µ—Ä–Ω–∏ –°–¢–†–û–ì–û –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π JSON —Å –ø–æ–ª—è–º–∏:
   "short_answer": –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ (‚â§200 —Å–∏–º–≤–æ–ª–æ–≤). –§–æ—Ä–º–∞—Ç–∏—Ä—É–π —Ç–∞–∫:
     - –µ—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ/–ª—å–≥–æ—Ç–∞ ‚Üí "–ó–∞–≤–∏—Å–∏—Ç: –µ—Å–ª–∏ <—É—Å–ª–æ–≤–∏–µ>, —Ç–æ <–∏—Ç–æ–≥>; –∏–Ω–∞—á–µ ‚Äî –ø–æ –æ–±—â–∏–º –ø—Ä–∞–≤–∏–ª–∞–º".
     - –∏–Ω–∞—á–µ ‚Üí "–ü–æ –æ–±—â–∏–º –ø—Ä–∞–≤–∏–ª–∞–º: <–∏—Ç–æ–≥>".
   "reasoned_answer": 1‚Äì3 –∫–æ—Ä–æ—Ç–∫–∏—Ö –∞–±–∑–∞—Ü–∞ (–≥–ª–∞–≤–Ω–∞—è –Ω–æ—Ä–º–∞ ‚Üí —Å–ø–µ—Ü-–∏—Å–∫–ª—é—á–µ–Ω–∏–µ ‚Üí –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥). –ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ—Ü–µ–¥—É—Ä, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤ —Ü–∏—Ç–∞—Ç–∞—Ö.
   "citations": –°–ü–ò–°–û–ö –æ–±—ä–µ–∫—Ç–æ–≤ {"punkt_num":"N","subpunkt_num":"M" –∏–ª–∏ "","quote":"—Ç–æ—á–Ω–∞—è –≤—ã–¥–µ—Ä–∂–∫–∞ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"}.
     –ú–∏–Ω–∏–º—É–º 1‚Äì2 —à—Ç. –°–Ω–∞—á–∞–ª–∞ –∫–ª—é—á–µ–≤–∞—è –Ω–æ—Ä–º–∞ –ø–æ —Å—É—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞.
   "related": –°–ü–ò–°–û–ö –æ–±—ä–µ–∫—Ç–æ–≤ {"punkt_num":"N","subpunkt_num":"M" –∏–ª–∏ ""} (–º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º []).

2) –¶–∏—Ç–∞—Ç—ã –±–µ—Ä—ë–º –¢–û–õ–¨–ö–û –∏–∑ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ï—Å–ª–∏ —Ç–æ—á–Ω—É—é —Ñ—Ä–∞–∑—É —Ç—Ä—É–¥–Ω–æ –≤—ã–¥–µ–ª–∏—Ç—å ‚Äî –ø—Ä–æ—Ü–∏—Ç–∏—Ä—É–π –∫–æ—Ä–æ—Ç–∫–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç (‚â§180/600 –∑–Ω–∞–∫–æ–≤ –ø–æ —Å–∏—Ç—É–∞—Ü–∏–∏).
3) –ù–ï —É–ø–æ–º–∏–Ω–∞–π –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å/–æ–ø–ª–∞—Ç—É/–û–ó–ü/–∫–æ–ª-–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –∏ —Ç.–ø., –µ—Å–ª–∏ –≠–¢–û –ù–ï –ø—Ä–æ—Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–æ.
4) –î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –ø—Ä–æ –∑–∞—Ä—É–±–µ–∂–Ω—É—é –º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä—É/–∏–Ω–æ—Å—Ç—Ä. –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ, –µ—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ (–Ω–∞–ø—Ä., –ø.32):
   ‚Äî –æ—Ç—Ä–∞–∑–∏ —ç—Ç–æ –≤ short_answer –≤ —Ñ–æ—Ä–º–∞—Ç–µ "–ó–∞–≤–∏—Å–∏—Ç: ‚Ä¶; –∏–Ω–∞—á–µ ‚Äî –ø–æ –æ–±—â–∏–º –ø—Ä–∞–≤–∏–ª–∞–º" –∏ –ø—Ä–æ—Ü–∏—Ç–∏—Ä—É–π –Ω–æ—Ä–º—É –∫–∞–∫ –ø–µ—Ä–≤—É—é –≤ "citations".
5) –î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –ø—Ä–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é (–º–æ–¥–µ—Ä–∞—Ç–æ—Ä/—ç–∫—Å–ø–µ—Ä—Ç/–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å/–º–∞—Å—Ç–µ—Ä) ‚Äî —Å—Ä–µ–¥–∏ "citations" –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ü–∏—Ç–∞—Ç–∞,
   –≥–¥–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–∞–∑–≤–∞–Ω–∞ —è–≤–Ω–æ, —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –ø–æ–¥–ø—É–Ω–∫—Ç–æ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø. 5.4 –¥–ª—è ¬´–ø–µ–¥–∞–≥–æ–≥–∞-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è¬ª).

5a) –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é ‚Äî –≤ "reasoned_answer" —Å–¥–µ–ª–∞–π –ö–û–†–û–¢–ö–ò–ô –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ (2‚Äì6 –ø—É–Ω–∫—Ç–æ–≤)
     –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π –ò–°–ö–õ–Æ–ß–ò–¢–ï–õ–¨–ù–û –∏–∑ –ø—Ä–æ—Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø.5.x (–±–µ–∑ –¥–æ–º—ã—Å–ª–æ–≤), –∑–∞—Ç–µ–º –æ–¥–∏–Ω –∞–±–∑–∞—Ü —Å –ø—Ä–æ—Ü–µ–¥—É—Ä–æ–π (–µ—Å–ª–∏ –ø—Ä–æ—Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø—É–Ω–∫—Ç—ã).
6) JSON ‚Äî –±–µ–∑ –ª–∏—à–Ω–∏—Ö –ø–æ–ª–µ–π, –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤, –∫–∞–≤—ã—á–∫–∏ —Ç–æ–ª—å–∫–æ –¥–≤–æ–π–Ω—ã–µ.
7) –ï—Å–ª–∏ –≤ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –ù–ï–¢ –Ω–æ—Ä–º—ã, –æ—Ç–≤–µ—á–∞—é—â–µ–π –ø—Ä—è–º–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å, —è–≤–Ω–æ –Ω–∞–ø–∏—à–∏, —á—Ç–æ –ø—Ä—è–º–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç; –Ω–µ –¥–æ–¥—É–º—ã–≤–∞–π –¥–µ—Ç–∞–ª–∏.
"""

def build_context_snippets(punkts: List[Dict[str, Any]], max_chars: int = 12000) -> str:
    def _window(txt: str, width_chars: int = 700) -> str:
        t = _collapse_repeats(txt or "").replace("\u00A0", " ")
        return t[:width_chars] + ("‚Ä¶" if len(t) > width_chars else "")
    parts: List[str] = []
    total = 0
    for p in punkts:
        pn = str(p.get("punkt_num") or "").strip()
        sp = str(p.get("subpunkt_num") or "").strip()
        head = f"–ø. {pn}{('.' + sp) if sp else ''}".strip()
        one = f"{head}: {_window(p.get('text',''))}"
        if total + len(one) + 2 > max_chars:
            break
        parts.append(one); total += len(one) + 2
    return "\n\n".join(parts)
def llm_rerank(question: str, punkts: List[Dict[str, Any]], top_n: int = 12) -> List[Dict[str, Any]]:
    if not LLM_RERANK or not punkts:
        return punkts[:top_n]
    # –ö—Ä–∞—Ç–∫–∏–µ –ø—Ä–µ–≤—å—é –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    items = []
    for p in punkts[:40]:
        pn = str(p.get("punkt_num","")).strip()
        sp = str(p.get("subpunkt_num","")).strip()
        pv = _collapse_repeats(p.get("text","")).split(". ")
        preview = ". ".join(pv[:2])[:300]
        items.append(f"{pn}.{sp or '‚Äì'}: {preview}")

    prompt = (
        "–í–æ–ø—Ä–æ—Å: " + question + "\n\n"
        "–ù–∏–∂–µ —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ ¬´–ø.–Ω–æ–º–µ—Ä: —Ñ—Ä–∞–≥–º–µ–Ω—Ç¬ª. –í–µ—Ä–Ω–∏ –°–¢–†–û–ì–ò–ô JSON —Å –º–∞—Å—Å–∏–≤–æ–º items, "
        "–≥–¥–µ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç: {\"punkt_num\":\"N\",\"subpunkt_num\":\"M –∏–ª–∏ ''\"}. "
        f"–í—ã–±–µ—Ä–∏ –Ω–µ –±–æ–ª–µ–µ {top_n} –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø–æ –≤–æ–ø—Ä–æ—Å—É, –ø–æ —É–±—ã–≤–∞–Ω–∏—é –≤–∞–∂–Ω–æ—Å—Ç–∏.\n\n"
        + "\n".join(items)
    )
    try:
        resp = call_with_retries(
            client.chat.completions.create,
            model=CHAT_MODEL,
            messages=[
                {"role":"system","content":"–¢—ã –≤—ã–±–∏—Ä–∞–µ—à—å —Å–∞–º—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–æ—Ä–º—ã –ü—Ä–∞–≤–∏–ª –∫ –≤–æ–ø—Ä–æ—Å—É. –ë–µ–∑ –¥–æ–º—ã—Å–ª–æ–≤."},
                {"role":"user","content": prompt},
            ],
            temperature=0,
            response_format={"type":"json_object"},
        )
        raw = json.loads(resp.choices[0].message.content or "{}")
        picked = raw.get("items") or []
        want = { (str(x.get("punkt_num","")).strip(), str(x.get("subpunkt_num","")).strip()) for x in picked if isinstance(x, dict) }
        if not want:
            return punkts[:top_n]
        out: List[Dict[str, Any]] = []
        for p in punkts:
            key = (str(p.get("punkt_num","")).strip(), str(p.get("subpunkt_num","")).strip())
            if key in want and p not in out:
                out.append(p)
            if len(out) >= top_n:
                break
        return out or punkts[:top_n]
    except Exception:
        return punkts[:top_n]
def narrow_punkts_by_intent(question: str, punkts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    info = classify_question(question)
    intent = info.get("intent", "general")
    cat = info.get("category")

    def _pn(p): return str(p.get("punkt_num","")).strip()
    def _sp(p): return str(p.get("subpunkt_num","")).strip()

    if intent == "commission":
        keys = ("–∫–æ–º–∏—Å—Å–∏", "—Å–æ—Å—Ç–∞–≤", "—á–ª–µ–Ω—ã –∫–æ–º–∏—Å")
        keep = [p for p in punkts if any(k in (p.get("text","").lower()) for k in keys)]
        return (keep or punkts)[:12]


    if intent == "fee":
        keep41 = [p for p in punkts if _pn(p) == "41"]
        keep10 = [p for p in punkts if _pn(p) == "10"][:1]
        return (keep41 + keep10 + [p for p in punkts if p not in keep41][:6])[:12] or punkts[:12]

    if intent == "publications":
        cat_pair = CAT_CANON.get(cat or "", ("",""))
        head = []
        if cat_pair[0]:
            head = [p for p in punkts if _pn(p)=="5" and _sp(p)==cat_pair[1]]
        five = [p for p in punkts if _pn(p)=="5" and p not in head][:6]
        ten  = [p for p in punkts if _pn(p)=="10"][:1]
        return (head + five + ten)[:12] or punkts[:12]

    if intent == "periodicity":
        keys = ("–Ω–µ —Ä–µ–∂–µ –æ–¥–Ω–æ–≥–æ —Ä–∞–∑–∞", "—Ä–∞–∑ –≤ –ø—è—Ç—å –ª–µ—Ç", "–∫–∞–∂–¥—ã–µ –ø—è—Ç—å –ª–µ—Ç", "–ø–µ—Ä–∏–æ–¥–∏—á")
        good = []
        for p in punkts:
            tl = (p.get("text") or "").lower().replace("—ë","–µ")
            if any(k in tl for k in keys):
                good.append(p)
        keep10 = [p for p in punkts if _pn(p)=="10"][:1]
        return (good + keep10)[:12] or punkts[:12]

    if intent == "threshold":
        p39 = [p for p in punkts if _pn(p)=="39"]
        p10 = [p for p in punkts if _pn(p)=="10"][:1]
        return (p39 + p10 + [p for p in punkts if p not in p39][:6])[:12] or punkts[:12]

    return punkts[:12]


def ask_llm(question: str, punkts: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    –°—Ç—Ä–æ–∏—Ç —Å—Ç—Ä–æ–≥–∏–π JSON-–æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –ü—Ä–∞–≤–∏–ª, —Å –ø–æ—Å—Ç-–∫–æ—ç—Ä—Å–∏–µ–π —Å—Ö–µ–º—ã.
    –ü—Ä–∏ —Å–ª–∞–±–æ–º –æ—Ç–≤–µ—Ç–µ ‚Äî –ø–æ–≤—Ç–æ—Ä–Ω—ã–π —É–∂–µ—Å—Ç–æ—á—ë–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å.
    """

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ helpers: —Å—Ö–µ–º–∞/–≤–∞–ª–∏–¥–∞—Ü–∏—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_user_prompt(template: str, q: str, context: str) -> str:
        return template.replace("{question}", q).replace("{context}", context)

    def _allowed_keys_set(punkts_: List[Dict[str, Any]]) -> set[Tuple[str, str]]:
        return {(str(p.get("punkt_num","")).strip(), str(p.get("subpunkt_num","")).strip()) for p in punkts_}

    def _closest_key(pn: str, sp: str, allowed: set[Tuple[str,str]]) -> Optional[Tuple[str,str]]:
        # —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        key = (pn, sp)
        if key in allowed:
            return key
        # –µ—Å–ª–∏ –ø–æ–¥–ø—É–Ω–∫—Ç –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º –±–µ–∑ –ø–æ–¥–ø—É–Ω–∫—Ç–∞
        key2 = (pn, "")
        if key2 in allowed:
            return key2
        return None

    _ALLOWED = _allowed_keys_set(punkts)

    _NUM_RE = re.compile(r"(\d{1,3})(?:\.(\d{1,3}))?")

    def _extract_pairs_from_text(s: str) -> List[Tuple[str,str]]:
        out = []
        for m in _NUM_RE.finditer(s or ""):
            pn = m.group(1) or ""
            sp = m.group(2) or ""
            if pn:
                key = _closest_key(pn, sp, _ALLOWED)
                if key:
                    out.append(key)
        # –¥–µ–¥—É–ø
        seen, uniq = set(), []
        for k in out:
            if k not in seen:
                seen.add(k); uniq.append(k)
        return uniq

    def _coerce_citations(raw: Any) -> List[Dict[str, str]]:
        """
        –ü—Ä–∏–≤–æ–¥–∏–º citations –∫ —Å–ø–∏—Å–∫—É –æ–±—ä–µ–∫—Ç–æ–≤ {punkt_num, subpunkt_num, quote}.
        –î–æ–ø—É—Å–∫–∞–µ–º –≤—Ö–æ–¥: —Å—Ç—Ä–æ–∫–∞ "–ø. 32", —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫, —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π c –ª—é–±—ã–º–∏ –ø–æ–ª—è–º–∏.
        –¶–∏—Ç–∞—Ç—ã-–≤—ã–¥–µ—Ä–∂–∫–∏ –ø–æ–¥—Å—Ç–∞–≤–∏–º –ø–æ–∑–∂–µ –≤ —Ä–µ–Ω–¥–µ—Ä–µ (validate_citations), –∑–¥–µ—Å—å –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É.
        """
        res: List[Dict[str,str]] = []
        if isinstance(raw, list):
            for item in raw:
                if isinstance(item, dict):
                    pn = str(item.get("punkt_num","")).strip()
                    sp = str(item.get("subpunkt_num","")).strip()
                    if not pn and isinstance(item.get("quote"), str):
                        # –ø–æ–ø—Ä–æ–±—É–µ–º –≤—ã–¥–µ—Ä–Ω—É—Ç—å –Ω–æ–º–µ—Ä –∏–∑ quote
                        pairs = _extract_pairs_from_text(item.get("quote",""))
                        if pairs:
                            pn, sp = pairs[0]
                    if pn:
                        key = _closest_key(pn, sp, _ALLOWED)
                        if key:
                            res.append({"punkt_num": key[0], "subpunkt_num": key[1], "quote": str(item.get("quote","")).strip()})
                elif isinstance(item, str):
                    for pn, sp in _extract_pairs_from_text(item):
                        res.append({"punkt_num": pn, "subpunkt_num": sp, "quote": ""})
        elif isinstance(raw, str):
            for pn, sp in _extract_pairs_from_text(raw):
                res.append({"punkt_num": pn, "subpunkt_num": sp, "quote": ""})
        # –¥–µ–¥—É–ø –ø–æ (pn,sp,quote)
        seen, uniq = set(), []
        for c in res:
            key = (c["punkt_num"], c["subpunkt_num"], c["quote"])
            if key not in seen:
                seen.add(key); uniq.append(c)
        return uniq

    def _coerce_related(raw: Any) -> List[Dict[str,str]]:
        res: List[Dict[str,str]] = []
        if isinstance(raw, list):
            for item in raw:
                if isinstance(item, dict):
                    pn = str(item.get("punkt_num","")).strip()
                    sp = str(item.get("subpunkt_num","")).strip()
                    if pn:
                        key = _closest_key(pn, sp, _ALLOWED)
                        if key:
                            res.append({"punkt_num": key[0], "subpunkt_num": key[1]})
                elif isinstance(item, str):
                    for pn, sp in _extract_pairs_from_text(item):
                        res.append({"punkt_num": pn, "subpunkt_num": sp})
        elif isinstance(raw, str):
            for pn, sp in _extract_pairs_from_text(raw):
                res.append({"punkt_num": pn, "subpunkt_num": sp})
        # –¥–µ–¥—É–ø –ø–æ (pn,sp)
        seen, uniq = set(), []
        for r in res:
            key = (r["punkt_num"], r["subpunkt_num"])
            if key not in seen:
                seen.add(key); uniq.append(r)
        return uniq

    def _normalize_llm_json(d: Dict[str, Any]) -> Dict[str, Any]:
        out = {
            "short_answer": str(d.get("short_answer","")).strip(),
            "reasoned_answer": str(d.get("reasoned_answer","")).strip(),
            "citations": _coerce_citations(d.get("citations", [])),
            "related": _coerce_related(d.get("related", [])),
        }
        return out

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ø–µ—Ä–≤–∏—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ø–µ—Ä–≤–∏—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    context_text = build_context_snippets(punkts)

    # –¥–æ–±–∞–≤–∏–º –≥–∞—Ä–¥: –µ—Å–ª–∏ must-have –Ω–æ—Ä–º—ã –∏–∑ –ø–æ–ª–∏—Ç–∏–∫–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ ‚Äî —Ç—Ä–µ–±—É–µ–º –æ—Ç LLM —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ –ø—Ä—è–º–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç
    intent_info = classify_question(question)
    must_pairs = policy_get_must_have_pairs(intent_info)
    have_keys = {(str(p.get("punkt_num","")).strip(), str(p.get("subpunkt_num","")).strip()) for p in punkts}
    missed = [pair for pair in must_pairs if pair not in have_keys]
    extra_guard = ""
    if must_pairs and missed:
        extra_guard = (
            "\n–í–ê–ñ–ù–û: –í –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –∫–ª—é—á–µ–≤–æ–π –Ω–æ—Ä–º—ã –ø–æ —Å—É—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞. "
            "–¢—ã –æ–±—è–∑–∞–Ω –≤–µ—Ä–Ω—É—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–π –≤—ã–≤–æ–¥ –≤–∏–¥–∞ "
            "\"–ü–æ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–º—É —Ñ—Ä–∞–≥–º–µ–Ω—Ç—É –ø—Ä—è–º–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç; –¥–µ–π—Å—Ç–≤—É–µ—Ç –æ–±—â–∏–π –ø–æ—Ä—è–¥–æ–∫\" "
            "–∏ –Ω–µ –¥–æ–¥—É–º—ã–≤–∞—Ç—å –¥–µ—Ç–∞–ª–∏."
        )

    user_prompt = _build_user_prompt(GEN_PROMPT_TEMPLATE + extra_guard, question, context_text)

    resp = call_with_retries(
        client.chat.completions.create,
        model=CHAT_MODEL,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0,
        response_format={"type": "json_object"},
    )
    raw_text = (resp.choices[0].message.content or "").strip()

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ø–∞—Ä—Å–∏–Ω–≥ + –∫–æ—ç—Ä—Å–∏—è —Å—Ö–µ–º—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ø–∞—Ä—Å–∏–Ω–≥ + –∫–æ—ç—Ä—Å–∏—è —Å—Ö–µ–º—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    try:
        data_raw = json.loads(raw_text)
    except Exception as e:
        logger.warning("LLM JSON decode error: %s; trying strict reprompt. Raw: %s", e, raw_text[:500])
        # –∂—ë—Å—Ç–∫–∏–π –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å—Ä–∞–∑—É, –µ—Å–ª–∏ JSON –Ω–µ —Ä–∞—Å–ø–∞—Ä—Å–∏–ª—Å—è
        extra = (
            "\n–í–ê–ñ–ù–û: –ü–µ—Ä–µ—Å–æ–±–µ—Ä–∏ –æ—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ –ø–æ —Å—Ö–µ–º–µ. "
            "citations ‚Äî —ç—Ç–æ –°–ü–ò–°–û–ö –û–ë–™–ï–ö–¢–û–í {punkt_num, subpunkt_num, quote}; "
            "related ‚Äî –°–ü–ò–°–û–ö –û–ë–™–ï–ö–¢–û–í {punkt_num, subpunkt_num}. "
            "–ú–∏–Ω–∏–º—É–º –¥–≤–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã –∏–∑ –ü–ï–†–ï–î–ê–ù–ù–û–ì–û –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."
        )
        strict_prompt = _build_user_prompt(GEN_PROMPT_TEMPLATE + extra, question, context_text)
        resp2 = call_with_retries(
            client.chat.completions.create,
            model=CHAT_MODEL,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": strict_prompt},
            ],
            temperature=0.1,
            response_format={"type": "json_object"},
        )
        raw_text2 = (resp2.choices[0].message.content or "").strip()
        try:
            data_raw = json.loads(raw_text2)
        except Exception as e2:
            logger.warning("LLM JSON decode error (strict) too: %s; raw: %s", e2, raw_text2[:500])
            return {
                "short_answer": "–ù–µ —É–¥–∞–ª–æ—Å—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç.",
                "reasoned_answer": "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–Ω–∞—á–µ –∏–ª–∏ —É—Ç–æ—á–Ω–∏—Ç–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç.",
                "citations": [],
                "related": [],
            }


    data = _normalize_llm_json(data_raw)

    # –±–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤ (–ø–æ—Å–ª–µ –∫–æ—ç—Ä—Å–∏–∏)
    if not isinstance(data["short_answer"], str) or not isinstance(data["reasoned_answer"], str) \
       or not isinstance(data["citations"], list) or not isinstance(data["related"], list):
        logger.warning("LLM schema still invalid after normalize; raw: %s", str(data_raw)[:500])
        return {
            "short_answer": "–ù–µ —É–¥–∞–ª–æ—Å—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç.",
            "reasoned_answer": "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–Ω–∞—á–µ –∏–ª–∏ —É—Ç–æ—á–Ω–∏—Ç–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç.",
            "citations": [],
            "related": [],
        }

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏: –Ω—É–∂–Ω–∞ –ª–∏ –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def _mentions_obligation(txt: str) -> bool:
        return bool(re.search(r"\b(–æ–±—è–∑–∞–Ω|–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ|–¥–æ–ª–∂–µ–Ω|–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ)\b", (txt or "").lower()))
    def _cit_pts(cits: List[Dict[str,str]]) -> set:
        return {c.get("punkt_num","") for c in (cits or [])}

    need_reask = False

    # –∏–Ω–æ—Å—Ç—Ä. –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ ‚Äî —Ö–æ—Ç–∏–º –≤–∏–¥–µ—Ç—å —Å–ø–µ—Ü-–Ω–æ—Ä–º—É
    ql = (question or "").lower()
    if any(k in ql for k in ("–º–∞–≥–∏—Å—Ç", "–∑–∞ —Ä—É–±–µ–∂", "–∑–∞ –≥—Ä–∞–Ω–∏—Ü", "–∑–∞—Ä—É–±–µ–∂", "–∏–Ω–æ—Å—Ç—Ä–∞–Ω", "–±–æ–ª–∞—à", "bolash", "nazarbayev", "nazarbayev university")):
        if "32" not in _cit_pts(data["citations"]):
            need_reask = True

    

    # —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–∏—Ç–∞—Ç?
    uniq_cits = {(c.get("punkt_num",""), c.get("subpunkt_num","")) for c in data["citations"]}
    if len([u for u in uniq_cits if u[0]]) < 2:
        need_reask = True

    # ¬´–æ–±—è–∑–∞–Ω/–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ‚Ä¶¬ª, –Ω–æ —Å—Ä–µ–¥–∏ —Ü–∏—Ç–∞—Ç –µ—Å—Ç—å 3/41 ‚Äî –ø–µ—Ä–µ—Å–ø—Ä–æ—Å–∏–º
    if _mentions_obligation(data["reasoned_answer"]) and (_cit_pts(data["citations"]) & {"3","41"}):
        need_reask = True

    if not need_reask:
        return data

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ —Å—Ç—Ä–æ–≥–∏–π –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    extra = (
        "\n–í–ê–ñ–ù–û: –ü–µ—Ä–µ—Å–æ–±–µ—Ä–∏ –æ—Ç–≤–µ—Ç.\n"
        "1) –î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –ø—Ä–æ –∑–∞—Ä—É–±–µ–∂–Ω—É—é –º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä—É –ø—Ä–æ—Ü–∏—Ç–∏—Ä—É–π —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø.32) –∏–∑ –ü–ï–†–ï–î–ê–ù–ù–û–ì–û –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞;\n"
        "2) –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π –ø.3/–ø.41 –∫–∞–∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏—é (–∏—Ö –º–æ–∂–Ω–æ —É–ø–æ–º–∏–Ω–∞—Ç—å —Ç–æ–ª—å–∫–æ –∫–∞–∫ —Å–ø—Ä–∞–≤–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é);\n"
        "3) –î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –ø—Ä–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é (–º–æ–¥–µ—Ä–∞—Ç–æ—Ä/—ç–∫—Å–ø–µ—Ä—Ç/–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å/–º–∞—Å—Ç–µ—Ä) ‚Äî –ø—Ä–æ—Ü–∏—Ç–∏—Ä—É–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, –≥–¥–µ —ç—Ç–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–∞–∑–≤–∞–Ω–∞ —è–≤–Ω–æ,\n"
        "   –∏ –∫—Ä–∞—Ç–∫–æ –ø–µ—Ä–µ—á–∏—Å–ª–∏ —ç—Ç–∞–ø—ã: –∑–∞—è–≤–ª–µ–Ω–∏–µ, –¥–æ–∫—É–º–µ–Ω—Ç—ã/–ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ, –∫—Ä–∏—Ç–µ—Ä–∏–∏/–±–∞–ª–ª—ã, —Å—Ä–æ–∫–∏, —Ä–µ—à–µ–Ω–∏–µ –∫–æ–º–∏—Å—Å–∏–∏;\n"
        "4) –°–æ—Ö—Ä–∞–Ω—è–π —Å—Ç—Ä–æ–≥—É—é —Å—Ö–µ–º—É: citations ‚Äî –°–ü–ò–°–û–ö –û–ë–™–ï–ö–¢–û–í {punkt_num, subpunkt_num, quote}; related ‚Äî –°–ü–ò–°–û–ö –û–ë–™–ï–ö–¢–û–í {punkt_num, subpunkt_num}.\n"
        "5) –ú–∏–Ω–∏–º—É–º –¥–≤–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã."
    )
    strict_prompt = _build_user_prompt(GEN_PROMPT_TEMPLATE + extra, question, context_text)

    resp2 = call_with_retries(
        client.chat.completions.create,
        model=CHAT_MODEL,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": strict_prompt},
        ],
        temperature=0,
        response_format={"type": "json_object"},
    )
    raw_text2 = (resp2.choices[0].message.content or "").strip()

    try:
        data_raw2 = json.loads(raw_text2)
        data2 = _normalize_llm_json(data_raw2)

        uniq_cits2 = {(c.get("punkt_num",""), c.get("subpunkt_num","")) for c in data2["citations"]}
        ok = (
            isinstance(data2["short_answer"], str)
            and isinstance(data2["reasoned_answer"], str)
            and isinstance(data2["citations"], list)
            and isinstance(data2["related"], list)
            and len([u for u in uniq_cits2 if u[0]]) >= 2
            and not (_mentions_obligation(data2["reasoned_answer"]) and (_cit_pts(data2["citations"]) & {"3","41"}))
        )
        if ok:
            return data2
    except Exception as e:
        logger.warning("LLM JSON parse (strict) error: %s; raw: %s", e, raw_text2[:500])

    # –µ—Å–ª–∏ –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞ –Ω–µ –ø–æ–º–æ–≥–ª–∞ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤–∏—á–Ω—ã–π –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π
    return data



def enforce_short_answer(question: str, data: dict, ctx_text: str) -> dict:
    import re
    sa = (data.get("short_answer") or "").strip()
    cites = {str(c.get("punkt_num","")) for c in (data.get("citations") or [])}
    ql = (question or "").lower()
    ctx = (ctx_text or "").lower()

    # –¥–µ—Ç–µ–∫—Ç–æ—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ —Å–∏–Ω–æ–Ω–∏–º–∞–º
    def _is_category_q(q: str) -> bool:
        qn = q.replace("—ë","–µ")
        for _, syns in CATEGORY_SYNONYMS.items():
            if any(s in qn for s in syns):
                return True
        return False

    foreign_trigger = any(t in ql for t in ("–º–∞–≥–∏—Å—Ç", "–∑–∞ —Ä—É–±–µ–∂", "–∑–∞—Ä—É–±–µ–∂", "–∏–Ω–æ—Å—Ç—Ä–∞–Ω", "–±–æ–ª–∞—à", "nazarbayev"))
    is_category_q = _is_category_q(ql)

    # –ï—Å–ª–∏ –µ—Å—Ç—å –ª—å–≥–æ—Ç–∞ (–ø.32 –≤ —Ü–∏—Ç–∞—Ç–∞—Ö) –∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ –∑–∞—Ä—É–±–µ–∂ ‚Äî —Å—Ç—Ä–æ–≥–∏–π —à–∞–±–ª–æ–Ω
    if foreign_trigger and ("32" in cites or "–±–µ–∑ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏" in ctx):
        sa = ("–ó–∞–≤–∏—Å–∏—Ç: –µ—Å–ª–∏ –º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä–∞ –æ–∫–æ–Ω—á–µ–Ω–∞ –≤ –∑–∞—Ä—É–±–µ–∂–Ω–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑ –ø–µ—Ä–µ—á–Ω—è ¬´–ë–æ–ª–∞—à–∞“õ¬ª, "
              "–∫–∞—Ç–µ–≥–æ—Ä–∏—è ¬´–ø–µ–¥–∞–≥–æ–≥-–º–æ–¥–µ—Ä–∞—Ç–æ—Ä¬ª –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç—Å—è –±–µ–∑ –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏; –∏–Ω–∞—á–µ ‚Äî –ø–æ –æ–±—â–∏–º –ø—Ä–∞–≤–∏–ª–∞–º.")
    else:
        if is_category_q and not sa.lower().startswith(("–ø–æ –æ–±—â–∏–º –ø—Ä–∞–≤–∏–ª–∞–º:", "–∑–∞–≤–∏—Å–∏—Ç:")):
            sa = "–ü–æ –æ–±—â–∏–º –ø—Ä–∞–≤–∏–ª–∞–º: " + sa

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ–¥–Ω–æ—Å–ª–æ–∂–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
    if re.fullmatch(r"\s*(–¥–∞|–Ω–µ—Ç)[\.\!]*\s*", sa, flags=re.I):
        sa = "–ü–æ –æ–±—â–∏–º –ø—Ä–∞–≤–∏–ª–∞–º: " + sa.capitalize()

    # –£–±–∏—Ä–∞–µ–º –∂—ë—Å—Ç–∫–∏–µ ¬´–æ–±—è–∑–∞–Ω/–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ¬ª, –µ—Å–ª–∏ –≤ —Ü–∏—Ç–∞—Ç–∞—Ö –µ—Å—Ç—å 3/41
    bad = {"3","41"}
    if (cites & bad):
        sa = re.sub(r"\b(–æ–±—è–∑–∞–Ω|–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ|–¥–æ–ª–∂–µ–Ω|–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ)\b", "—Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ –æ–±—â–∏–º –Ω–æ—Ä–º–∞–º", sa, flags=re.I)

    # –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏ —Å—Ä–µ–¥–∏ —Ü–∏—Ç–∞—Ç –µ—Å—Ç—å –ø.10 ‚Äî –¥–æ–±–∞–≤–∏–º —ç—Ç–∞–ø—ã (–µ—Å–ª–∏ –≤–ª–µ–∑–∞—é—Ç –≤ –ª–∏–º–∏—Ç)
    if is_category_q and "10" in cites and "‚Üí" not in sa:
        tail = " (—ç—Ç–∞–ø—ã: –∑–∞—è–≤–ª–µ–Ω–∏–µ ‚Üí –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ ‚Üí –û–ó–ü ‚Üí –æ–±–æ–±—â–µ–Ω–∏–µ ‚Üí —Ä–µ—à–µ–Ω–∏–µ –∫–æ–º–∏—Å—Å–∏–∏)"
        if len(sa) + len(tail) <= 200:
            sa += tail
    # –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è ‚Äî –¥–æ–±–∞–≤–∏–º —Å—Å—ã–ª–∫—É –Ω–∞ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π –ø–æ–¥–ø—É–Ω–∫—Ç, –µ—Å–ª–∏ –≤–ª–µ–∑–∞–µ—Ç
    if is_category_q:
        target = None
        for key, syns in CATEGORY_SYNONYMS.items():
            if any(s in ql for s in syns):
                target = key
                break
        if target:
            pn, sp = CAT_CANON.get(target, ("",""))
            tag = f" (—Å–º. –ø. {pn}.{sp})" if pn and sp else ""
            if tag and "—Å–º. –ø." not in sa and len(sa) + len(tag) <= 200:
                sa += tag


    data["short_answer"] = sa[:200]
    return data


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ü–æ—Å—Ç-–ø—Ä–æ—Ü–µ—Å—Å –∏ —Ä–µ–Ω–¥–µ—Ä ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _collapse_repeats(text: str) -> str:
    """
    –£–¥–∞–ª—è–µ—Ç –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏–µ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏, —Å–∂–∏–º–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
    –∏ –≤—ã—á–∏—â–∞–µ—Ç ¬´–º—É—Å–æ—Ä–Ω—ã–µ¬ª —Å—Ç—Ä–æ–∫–∏ –≤–∏–¥–∞ '1.2.' / '2.3.4.' / –ø—É—Å—Ç—ã–µ –º–∞—Ä–∫–µ—Ä—ã.
    """
    lines = [ln.rstrip() for ln in (text or "").splitlines()]
    out = []
    prev = None
    for ln in lines:
        s = ln.strip()
        if not s:
            continue
        # –≤—ã–∫–∏–¥—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫–∏, —Å–æ—Å—Ç–æ—è—â–∏–µ —Ç–æ–ª—å–∫–æ –∏–∑ –Ω—É–º–µ—Ä–∞—Ü–∏–∏ (1.2., 3.4.5. –∏ —Ç.–ø.)
        if re.fullmatch(r"\d+(?:\.\d+){0,4}\.?", s):
            continue
        if s == prev:
            continue
        out.append(s)
        prev = s
    s = "\n".join(out)
    # –∑–∞—â–∏—Ç–∞ –æ—Ç –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω—ã—Ö –ø–æ–≤—Ç–æ—Ä–æ–≤ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π —Ñ—Ä–∞–∑—ã –≤ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ
    s = re.sub(r"(–û—Ü–µ–Ω–∏–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤[^\.]*\.)\s*(\1\s*)+", r"\1 ", s, flags=re.I)
    # –º—è–≥–∫–æ —É–±–∏—Ä–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ ¬´–≤–∫—Ä–∞–ø–ª–µ–Ω–∏—è¬ª –Ω—É–º–µ—Ä–∞—Ü–∏–∏ –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–æ–∫–∏
    s = re.sub(r"(?<=\s)\d+(?:\.\d+){1,4}\.?(?=\s|$)", "", s)
    return re.sub(r"[ \t]+", " ", s).strip()


def validate_citations(citations: List[Dict[str, Any]],
                       punkts: List[Dict[str, Any]],
                       allow_p41: bool = False) -> List[Dict[str, Any]]:
    SKIP_AS_EVIDENCE = {"3"} if allow_p41 else {"3","41"}

    allowed_keys = {(str(p.get("punkt_num","")).strip(), str(p.get("subpunkt_num","")).strip()) for p in punkts}
    by_key = {
        (str(p.get("punkt_num","")).strip(), str(p.get("subpunkt_num","")).strip()): (p.get("text") or "")
        for p in punkts
    }

    out: List[Dict[str, Any]] = []
    for c in (citations or []):
        pn = str(c.get("punkt_num","")).strip()
        sp = str(c.get("subpunkt_num","")).strip()
        if not pn or pn in SKIP_AS_EVIDENCE or (pn, sp) not in allowed_keys:
            continue
        base = by_key.get((pn, sp), "")
        if not base.strip():
            continue

        limit = QUOTE_WIDTH_LONG if pn == "5" else QUOTE_WIDTH_DEFAULT
        qt = (c.get("quote") or "")
        base_clean = _collapse_repeats(base)
        if qt and qt.lower().replace("—ë","–µ") in base.lower().replace("—ë","–µ"):
            qt_clean = _collapse_repeats(qt)
            good = qt_clean if len(qt_clean) <= limit else (qt_clean[:limit] + "‚Ä¶")
        else:
            good = base_clean if len(base_clean) <= limit else (base_clean[:limit] + "‚Ä¶")

        out.append({"punkt_num": pn, "subpunkt_num": sp, "quote": good})

    seen: set[Tuple[str,str,str]] = set(); uniq: List[Dict[str, Any]] = []
    for c in out:
        key = (c["punkt_num"], c["subpunkt_num"], c["quote"])
        if key not in seen:
            seen.add(key); uniq.append(c)
    return uniq[:8]

async def handle_debug_webhook(request: web.Request) -> web.Response:
    try:
        r = requests.get(f"{TELEGRAM_API}/getWebhookInfo", timeout=15)
        return web.json_response(r.json(), status=r.status_code)
    except Exception as e:
        logger.exception("getWebhookInfo failed")
        return web.json_response({"error": str(e)}, status=500)


def _ensure_category_citation(question: str,
                              citations: List[Dict[str, Any]],
                              punkts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    ql = (question or "").lower().replace("—ë", "–µ")

    # –∏—â–µ–º —Ü–µ–ª–µ–≤—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ —Å–∏–Ω–æ–Ω–∏–º–∞–º
    target = None
    for key, syns in CATEGORY_SYNONYMS.items():
        if any(s in ql for s in syns):
            target = key
            break
    if not target:
        return citations

    # —É–∂–µ –µ—Å—Ç—å —Ü–∏—Ç–∞—Ç–∞ —Å –Ω—É–∂–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π?
    by_key_txt = {
        (str(p.get("punkt_num","")).strip(), str(p.get("subpunkt_num","")).strip()):
            (p.get("text") or "").lower().replace("—ë", "–µ")
        for p in punkts
    }
    def _mentions_cat(c: Dict[str, Any]) -> bool:
        key = (str(c.get("punkt_num","")).strip(), str(c.get("subpunkt_num","")).strip())
        return target in by_key_txt.get(key, "")

    if any(_mentions_cat(c) for c in (citations or [])):
        return citations

    # –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π –ø.5.x –ø–µ—Ä–≤—ã–º, –µ—Å–ª–∏ –æ–Ω –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
    pn, sp = CAT_CANON.get(target, ("", ""))
    for p in punkts:
        if str(p.get("punkt_num","")).strip()==pn and str(p.get("subpunkt_num","")).strip()==sp:
            return [{"punkt_num": pn, "subpunkt_num": sp, "quote": ""}] + (citations or [])
    # –≤ _ensure_category_citation(...)
    for p in punkts:
        tl = (p.get("text") or "").lower().replace("—ë","–µ")
        if p.get("punkt_num")=="5" and target in tl:
            return [{"punkt_num":"5","subpunkt_num": str(p.get("subpunkt_num","")).strip(), "quote": ""}] + (citations or [])

    # –∏–Ω–∞—á–µ ‚Äî –¥–æ–±–∞–≤–∏–º –ø–µ—Ä–≤—ã–π –ø—É–Ω–∫—Ç –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –≥–¥–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –∫–æ—Ä–µ–Ω—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    for p in punkts:
        txt = (p.get("text") or "").lower().replace("—ë", "–µ")
        if target in txt:
            return [{"punkt_num": str(p.get("punkt_num","")).strip(),
                     "subpunkt_num": str(p.get("subpunkt_num","")).strip(),
                     "quote": ""}] + (citations or [])
    return citations

# ‚îÄ‚îÄ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ü–∏—Ç–∞—Ç –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏–∑ –≤–æ–ø—Ä–æ—Å–∞ ‚îÄ‚îÄ
def filter_citations_by_question(
    question: str,
    citations: List[Dict[str, Any]],
    punkts: List[Dict[str, Any]],
    intent: str = "general"
) -> List[Dict[str, Any]]:
    import re

    ql = (question or "").lower().replace("—ë", "–µ")
    if not citations:
        return citations

    by_key_full = {
        (str(p.get("punkt_num","")).strip(), str(p.get("subpunkt_num","")).strip()): (p.get("text") or "")
        for p in punkts
    }
    by_key = {k: v.lower().replace("—ë","–µ") for k, v in by_key_full.items()}

    def _crop_around(text_full: str, keys, width: int = QUOTE_WIDTH_DEFAULT) -> str:
        tf = re.sub(r"[ \t]+", " ", text_full or "").strip().replace("\u00A0", " ")
        tl = tf.lower().replace("—ë", "–µ")
        pos = -1
        for k in keys or ():
            i = tl.find(k)
            if i != -1 and (pos == -1 or i < pos):
                pos = i
        if pos == -1:
            return tf[:width] + ("‚Ä¶" if len(tf) > width else "")
        pad = width // 2
        start = max(0, pos - pad); end = min(len(tf), pos + pad)
        while start > 0 and tf[start] not in " .,;:!?()[]{}¬´¬ª": start -= 1
        while end < len(tf) and tf[end - 1] not in " .,;:!?()[]{}¬´¬ª": end += 1
        snippet = tf[start:end].strip()
        snippet = snippet.lstrip(" ;,.:‚Äî-‚Äì‚Ä¢").rstrip(" ,;:")
        return snippet[:width] + ("‚Ä¶" if len(snippet) > width else "")

    # remove 3/(41) ‚Äî –¥–ª—è fee –æ—Å—Ç–∞–≤–ª—è–µ–º 41
    remove = {"3"} if intent == "fee" else {"3","41"}
    clean = [c for c in citations if str(c.get("punkt_num","")).strip() not in remove]
    if not clean:
        clean = citations[:]
    # remove 3/(41) ‚Äî –¥–ª—è fee –æ—Å—Ç–∞–≤–ª—è–µ–º 41
    remove = {"3"} if intent == "fee" else {"3","41"}
    clean = [c for c in citations if str(c.get("punkt_num","")).strip() not in remove]
    if not clean:
        clean = citations[:]

    # üîΩ –î–û–ë–ê–í–ò–¢–¨: –Ω–µ —Ç–∞—â–∏–º –ø.39, –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ –ø—Ä–æ –ø–æ—Ä–æ–≥/–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    if intent not in {"threshold", "category_requirements"}:
        clean = [
            c for c in clean
            if str(c.get("punkt_num","")).strip() != "39"
               or any(k in ql for k in KW_OZP_TERMS)
        ]
    # üîº

    # foreign?
    if intent == "exemption_foreign":
        p32 = [c for c in clean if str(c.get("punkt_num","")).strip() == "32"]
        rest = [c for c in clean if str(c.get("punkt_num","")).strip() != "32"]
        pref_terms = ("—É—á–µ–Ω", "—É—á—ë–Ω–∞—è", "—Å—Ç–µ–ø–µ–Ω", "phd", "–∫–∞–Ω–¥–∏–¥", "–¥–æ–∫—Ç–æ—Ä", "–ø–µ—Ä–µ—á–µ–Ω—å —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã—Ö", "nazarbayev", "–±–æ–ª–∞—à", "–±–µ–∑ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è")
        ordered = p32 + rest
        out = (ordered[:2] or clean[:2])
        for c in out:
            key = (str(c.get("punkt_num","")).strip(), str(c.get("subpunkt_num","")).strip())
            base_full = by_key_full.get(key, "")
            if base_full:
                c["quote"] = _collapse_repeats(_crop_around(base_full, pref_terms, width=QUOTE_WIDTH_DEFAULT))
        return out

    # retirement?
    if intent == "exemption_retirement":
        p30 = [c for c in clean if str(c.get("punkt_num","")).strip() == "30"]
        p57 = [c for c in clean if str(c.get("punkt_num","")).strip() == "57"]
        rest = [c for c in clean if c not in (p30 + p57)]
        out = (p30 + p57 + rest)[:2]
        for c in out:
            key = (str(c.get("punkt_num","")).strip(), str(c.get("subpunkt_num","")).strip())
            base_full = by_key_full.get(key, "")
            if base_full:
                c["quote"] = _collapse_repeats(_crop_around(base_full, ("–ø–µ–Ω—Å–∏–æ–Ω","–æ—Å–≤–æ–±–æ–∂–¥–∞","–æ–±–æ–±—â–µ–Ω","–æ–∑–ø"), width=QUOTE_WIDTH_DEFAULT))
        return out

    # category?
    target = None
    for key, syns in CATEGORY_SYNONYMS.items():
        if any(s in ql for s in syns):
            target = key
            break

    if target:
        canon_sp = CAT_CANON.get(target, ("",""))[1]
        def _ord(c):
            pn = str(c.get("punkt_num","")).strip()
            sp = str(c.get("subpunkt_num","")).strip()
            if pn == "5" and sp == canon_sp: return (0, 0)
            if pn == "5": return (1, 0)
            if pn == "10": return (2, 0)
            if pn == "39": return (3, 0)
            return (4, 0)

        clean.sort(key=_ord)
        out = clean[:3]

        have = {(str(c.get("punkt_num","")).strip(), str(c.get("subpunkt_num","")).strip()) for c in out}
        for pn in ("10","39"):
            if not any(h[0] == pn for h in have):
                for (kpn, ksp), txt in by_key_full.items():
                    if kpn == pn:
                        out.append({"punkt_num": kpn, "subpunkt_num": ksp, "quote": ""}); break

        human = CATEGORY_LABEL.get(target, target)
        cat_keys = tuple({
            f"–ø–µ–¥–∞–≥–æ–≥-{target}", f"–ø–µ–¥–∞–≥–æ–≥ {target}",
            f"–ø–µ–¥–∞–≥–æ–≥-{human}", f"–ø–µ–¥–∞–≥–æ–≥ {human}",
            target, human
        })

        for c in out:
            key = (str(c.get("punkt_num","")).strip(), str(c.get("subpunkt_num","")).strip())
            base_full = by_key_full.get(key, "")
            if not base_full:
                continue
            pn = key[0]
            if pn == "5":
                width = QUOTE_WIDTH_LONG
                snippet = _crop_around(base_full, cat_keys, width=width)
                tl = snippet.lower().replace("—ë","–µ")
                if not any(k in tl for k in cat_keys):
                    for sent in re.split(r"(?<=[\.\!\?])\s+", base_full):
                        sl = (sent or "").lower().replace("—ë","–µ")
                        if any(k in sl for k in cat_keys):
                            snippet = sent[:width] + ("‚Ä¶" if len(sent) > width else ""); break
                c["quote"] = _collapse_repeats(snippet)
            elif pn == "10":
                keys10 = ("–∑–∞—è–≤–ª–µ–Ω", "–ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ", "–æ–∑–ø", "–æ–±–æ–±—â–µ–Ω", "–∫–æ–º–∏—Å—Å–∏")
                c["quote"] = _collapse_repeats(_crop_around(base_full, keys10, width=QUOTE_WIDTH_DEFAULT))
            elif pn == "39":
                perc = (extract_threshold_percent_from_p39_for_category(punkts, target)
                        or extract_threshold_percent_from_p39(punkts))
                keys39 = ()
                if perc:
                    num = re.search(r"\d{1,3}", perc).group(0)
                    keys39 = (perc, f"{num} %", f"{num}\u00A0%")
                c["quote"] = _collapse_repeats(_crop_around(base_full, keys39, width=QUOTE_WIDTH_DEFAULT))
            else:
                c["quote"] = _collapse_repeats(_crop_around(base_full, tuple(), width=QUOTE_WIDTH_DEFAULT))
        return out[:3]
        # publications?
    if intent == "publications":
        out = clean[:2]
        keys_pub = ("–ø—É–±–ª–∏–∫–∞—Ü","–∂—É—Ä–Ω–∞–ª","—Å—Ç–∞—Ç","scopus","web of science","wos","doi","–∏–Ω–¥–µ–∫—Å–∏—Ä","—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω")
        for c in out:
            key = (str(c.get("punkt_num","")).strip(), str(c.get("subpunkt_num","")).strip())
            base_full = by_key_full.get(key, "")
            if base_full:
                c["quote"] = _collapse_repeats(_crop_around(base_full, keys_pub, width=QUOTE_WIDTH_DEFAULT if key[0]!="5" else QUOTE_WIDTH_LONG))
        return out

    # fee ‚Äî –ø—Ä–æ—Å—Ç–æ –¥–æ 2 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö, –Ω–µ –≤—ã–∫–∏–¥—ã–≤–∞—è 41
    if intent == "fee":
        out = clean[:2]
        for c in out:
            key = (str(c.get("punkt_num","")).strip(), str(c.get("subpunkt_num","")).strip())
            base_full = by_key_full.get(key, "")
            if base_full:
                c["quote"] = _collapse_repeats(_crop_around(base_full, ("–æ–ø–ª–∞—Ç","–ø–ª–∞—Ç", "—Å—Ç–æ–∏–º–æ—Å—Ç", "–±–µ—Å–ø–ª–∞—Ç"), width=QUOTE_WIDTH_DEFAULT))
        return out

    # default
    out = clean[:3]
    for c in out:
        key = (str(c.get("punkt_num","")).strip(), str(c.get("subpunkt_num","")).strip())
        base_full = by_key_full.get(key, "")
        if base_full:
            c["quote"] = _collapse_repeats(_crop_around(base_full, tuple(), width=QUOTE_WIDTH_DEFAULT))
    return out


def enforce_reasoned_answer(question: str, data: Dict[str, Any], punkts: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    –î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –ø—Ä–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—é:
    ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–π –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ (2‚Äì6 –ø—É–Ω–∫—Ç–æ–≤) –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π –∏–∑ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–≥–æ –ø.5.x;
    ‚Äî –∑–∞—Ç–µ–º –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ –ø—Ä–æ –ø—Ä–æ—Ü–µ–¥—É—Ä—É (–ø.10), –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ/—Ü–∏—Ç–∞—Ç–∞—Ö.
    """
    ql = (question or "").lower().replace("—ë", "–µ")

    # –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é
    target = None
    for key, syns in CATEGORY_SYNONYMS.items():
        if any(s in ql for s in syns):
            target = key; break
    if not target:
        return data

    # —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–∞—è –º–µ—Ç–∫–∞
    human = CATEGORY_LABEL.get(target, target)

    # –Ω–∞–π—Ç–∏ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π –ø.5.x
    pn, sp = CAT_CANON.get(target, ("",""))
    base_txt = ""
    for p in punkts:
        if str(p.get("punkt_num","")).strip()==pn and str(p.get("subpunkt_num","")).strip()==sp:
            base_txt = p.get("text","") or ""
            break
    if not base_txt:
        return data

    def _bullets_from(text: str, max_items: int = 6) -> List[str]:
        t = _collapse_repeats(text)
        parts = re.split(r"(?:\n+|‚Ä¢|‚Äî|\u2014|;|\.\s+|\d+\)|\d+\.)", t)
        parts = [re.sub(r"[ \t]+"," ", s).strip(" -‚Äî‚Ä¢.;") for s in parts]
        parts = [s for s in parts if 20 <= len(s) <= 240]
        uniq, seen = [], set()
        for s in parts:
            key = s.lower()[:80]
            if key not in seen:
                seen.add(key); uniq.append(s)
            if len(uniq) >= max_items:
                break
        return uniq[:max_items]

    bullets = _bullets_from(base_txt)
    if not bullets:
        return data

    cites = {str(c.get("punkt_num","")).strip() for c in (data.get("citations") or [])}
    have_p10 = ("10" in cites) or any(str(p.get("punkt_num","")).strip()=="10" for p in punkts)

    lines = [f"–ö–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ (¬´–ø–µ–¥–∞–≥–æ–≥-{human}¬ª, –ø.{pn}.{sp}):"]
    lines += [f"‚Äî {b}" for b in bullets[:6]]
    if have_p10:
        lines.append("–ü—Ä–æ—Ü–µ–¥—É—Ä–∞: –∑–∞—è–≤–ª–µ–Ω–∏–µ ‚Üí –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ ‚Üí –û–ó–ü ‚Üí –æ–±–æ–±—â–µ–Ω–∏–µ ‚Üí —Ä–µ—à–µ–Ω–∏–µ –∫–æ–º–∏—Å—Å–∏–∏ (–ø.10).")

    current = (data.get("reasoned_answer") or "").strip()
    if len(current) < 60 or "‚Äî " not in current:
        data["reasoned_answer"] = "\n".join(lines)
    else:
        data["reasoned_answer"] = current + "\n\n" + "\n".join(lines)
    return data
# 1) –û–±—â–∏–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –ø—Ä–æ—Ü–µ–Ω—Ç–∞ –ø–æ—Ä–æ–≥–∞ –∏–∑ –ø.39 (fallback –¥–ª—è –ª—é–±—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π)
def extract_threshold_percent_from_p39(punkts: List[Dict[str, Any]]) -> Optional[str]:
    """
    –ò—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ –ø.39 –±–ª–∏–∂–∞–π—à–∏–π –≤–∏–¥ 'NN %' –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º:
      - –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ 80%, –µ—Å–ª–∏ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è;
      - –∏–Ω–∞—á–µ –º–∞–∫—Å–∏–º—É–º –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤;
      - –∏–Ω–∞—á–µ None.
    """
    text39 = None
    for p in punkts:
        if str(p.get("punkt_num", "")).strip() == "39":
            text39 = (p.get("text") or "")
            break
    if not text39:
        return None

    tl = text39.lower().replace("—ë", "–µ")
    nums = [int(x) for x in re.findall(r"(\d{1,3})\s*%", tl)]
    if not nums:
        return None
    if 80 in nums:
        return "80%"
    return f"{max(nums)}%"
# 2) –ü–æ–ª–∏—Ç–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ "reasoned_answer" –ø–æ –ø–æ–ª–∏—Ç–∏–∫–∞–º (–ª—å–≥–æ—Ç—ã/–ø–µ–Ω—Å–∏–æ–Ω–µ—Ä—ã + long_template)
def enforce_policy_reasoned_answer(question: str,
                                   data: Dict[str, Any],
                                   intent_info: Dict[str, Any]) -> Dict[str, Any]:
    import re
    intent = intent_info.get("intent", "general")
    policy = POLICIES.get(intent, {})
    long_t = policy.get("long_template")
    ra = (data.get("reasoned_answer") or "").strip()

    # —Å–º—è–≥—á–∞–µ–º —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –¥–ª—è –ª—å–≥–æ—Ç–Ω—ã—Ö –∫–µ–π—Å–æ–≤
    if intent in {"exemption_foreign", "exemption_retirement"}:
        ra = re.sub(r"\b(–æ–±—è–∑–∞–Ω|–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ|–¥–æ–ª–∂–µ–Ω|–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ)\b",
                    "–ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –ø–æ—Ä—è–¥–æ–∫, –ø—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã–π –ü—Ä–∞–≤–∏–ª–∞–º–∏",
                    ra, flags=re.I)

    # –µ—Å–ª–∏ —É –ø–æ–ª–∏—Ç–∏–∫–∏ –µ—Å—Ç—å –¥–ª–∏–Ω–Ω—ã–π —à–∞–±–ª–æ–Ω ‚Äî —Å—Ç–∞–≤–∏–º –µ–≥–æ –∫–∞–∫ –æ—Å–Ω–æ–≤—É
    if long_t:
        data["reasoned_answer"] = long_t + (("\n\n" + ra) if ra else "")
    else:
        data["reasoned_answer"] = ra

    return data

# ‚îÄ‚îÄ –í—Å—Ç–∞–≤–∏—Ç—å —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ enforce_policy_reasoned_answer ‚îÄ‚îÄ
def sanitize_numeric_claims(data: Dict[str, Any]) -> Dict[str, Any]:
    """
    –£–±–∏—Ä–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∑–∞—è–≤–ª–µ–Ω–∏—è (–Ω–∞–ø—Ä. ¬´7 –±–∞–ª–ª–æ–≤¬ª) –∏–∑ reasoned/short,
    –µ—Å–ª–∏ —Ç–∞–∫–∏–µ —á–∏—Å–ª–∞ –Ω–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ –ø—Ä–æ—Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç—Ä—ã–≤–∫–∞—Ö.
    –û—Å—Ç–∞–≤–ª—è–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã –¥–ª—è threshold/category.
    """
    ra = (data.get("reasoned_answer") or "")
    sa = (data.get("short_answer") or "")
    ctext = " ".join([c.get("quote","") for c in (data.get("citations") or [])]).lower()

    def _clean_numbers(txt: str) -> str:
        txt = re.sub(r"(\b)(\d{1,3})\s*(–±–∞–ª–ª[–∞-—è]*)",
                     lambda m: (m.group(1) + m.group(3)) if m.group(2) not in ctext else m.group(0),
                     txt, flags=re.I)
        txt = re.sub(r"(\b)(\d{1,3})\s*(–ª–µ—Ç|–≥–æ–¥–∞)",
                     lambda m: (m.group(1) + m.group(3)) if m.group(2) not in ctext else m.group(0),
                     txt, flags=re.I)
        return txt

    data["reasoned_answer"] = _clean_numbers(ra)
    data["short_answer"] = _clean_numbers(sa)
    return data


def split_for_telegram(text: str, limit: int = 4000) -> List[str]:
    parts: List[str] = []
    buf: List[str] = []
    cur = 0
    for line in (text or "").split("\n"):
        if cur + len(line) + 1 > limit:
            parts.append("\n".join(buf))
            buf = [line]
            cur = len(line) + 1
        else:
            buf.append(line)
            cur += len(line) + 1
    if buf:
        parts.append("\n".join(buf))
    return parts

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Telegram I/O ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def tg_send_message(chat_id: int, text: str, parse_mode: str = "HTML", reply_markup: Optional[dict] = None) -> Optional[int]:
    url = f"{TELEGRAM_API}/sendMessage"
    payload = {
        "chat_id": chat_id,
        "text": text,
        "parse_mode": parse_mode,
        "disable_web_page_preview": True,
    }
    if reply_markup is not None:
        payload["reply_markup"] = reply_markup

    r = requests.post(url, json=payload, timeout=15)
    try:
        if r.ok:
            return r.json().get("result", {}).get("message_id")
        # fallback: —É–±—Ä–∞—Ç—å HTML –ø—Ä–∏ parse error
        if "can't parse entities" in r.text.lower():
            payload.pop("parse_mode", None)
            r2 = requests.post(url, json=payload, timeout=15)
            if r2.ok:
                return r2.json().get("result", {}).get("message_id")
        logger.error("sendMessage failed: %s %s", r.status_code, r.text)
    except Exception:
        logger.exception("sendMessage decode error")
    return None



def tg_edit_message_text(chat_id: int, message_id: int, text: str, parse_mode: str = "HTML", reply_markup: Optional[dict] = None) -> None:
    url = f"{TELEGRAM_API}/editMessageText"
    payload = {
        "chat_id": chat_id,
        "message_id": message_id,
        "text": text,
        "parse_mode": parse_mode,
        "disable_web_page_preview": True,
    }
    if reply_markup is not None:
        payload["reply_markup"] = reply_markup

    r = requests.post(url, json=payload, timeout=15)
    if r.ok:
        return
    if "can't parse entities" in r.text.lower():
        payload.pop("parse_mode", None)
        r2 = requests.post(url, json=payload, timeout=15)
        if r2.ok:
            return
    logger.error("editMessageText failed: %s %s", r.status_code, r.text)


def kb_show_detailed():
    return {"inline_keyboard": [[{"text": "–ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç–≤–µ—Ç", "callback_data": "show_detailed"}]]}

def kb_show_short():
    return {"inline_keyboard": [[{"text": "–ü–æ–∫–∞–∑–∞—Ç—å –∫—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç", "callback_data": "show_short"}]]}
def render_short_html(question: str, data: Dict[str, Any]) -> str:
    sa = html.escape(data.get("short_answer", "")).strip()
    ra = html.escape(data.get("reasoned_answer", "")).strip()
    lines = [f"<b>–í–æ–ø—Ä–æ—Å:</b> {html.escape(question)}"]
    if sa:
        lines.append(f"<b>–ö—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç:</b>\n{sa}")
    # –Ω–∞–º—ë–∫, —á—Ç–æ –µ—Å—Ç—å –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏
    if ra:
        lines.append("<i>–ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –Ω–∏–∂–µ, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –∏ —Ü–∏—Ç–∞—Ç—ã.</i>")
    return "\n".join(lines)

# –ò–Ω—Ç–µ–Ω—Ç—ã, –≥–¥–µ ¬´–°–≤—è–∑–∞–Ω–Ω—ã–µ –ø—É–Ω–∫—Ç—ã¬ª –ª—É—á—à–µ —Å–∫—Ä—ã—Ç—å, —á—Ç–æ–±—ã –Ω–µ —à—É–º–µ—Ç—å
INTENTS_HIDE_RELATED = {"threshold", "exemption_foreign", "exemption_retirement", "fee", "periodicity"}



def render_detailed_html(question: str, data: Dict[str, Any], punkts: List[Dict[str, Any]]) -> str:
    sa = html.escape(data.get("short_answer", "")).strip()
    ra = html.escape(data.get("reasoned_answer", "")).strip()

    intent_info = classify_question(question)
    intent = intent_info.get("intent", "general")

    data["citations"] = _ensure_category_citation(question, data.get("citations", []), punkts)

    citations = validate_citations(data.get("citations", []), punkts, allow_p41=(intent=="fee"))
    citations = filter_citations_by_question(question, citations, punkts, intent=intent)

    have_cit = {(str(c.get("punkt_num","")).strip(), str(c.get("subpunkt_num","")).strip()) for c in citations}
    related = data.get("related", []) or []

    # related: –±–µ–∑ —à—É–º–∞. –ø.39 ‚Äî —Ç–æ–ª—å–∫–æ –¥–ª—è threshold/–∫–∞—Ç–µ–≥–æ—Ä–∏–π; –ø.63 ‚Äî —Ç–æ–ª—å–∫–æ –¥–ª—è ¬´commission¬ª
    def _exists(pn: str, sp: str = "") -> bool:
        for p in punkts:
            if str(p.get("punkt_num","")).strip()==pn and (sp=="" or str(p.get("subpunkt_num","")).strip()==sp):
                return True
        return False
    def _push(pn: str, sp: str = "") -> None:
        if (pn, sp) not in have_cit:
            related.append({"punkt_num": pn, "subpunkt_num": sp})

    if intent in {"threshold", "category_requirements"} and _exists("39"):
        _push("39","")

    
    data["related"] = related

    lines: List[str] = []
    lines.append(f"<b>–í–æ–ø—Ä–æ—Å:</b> {html.escape(question)}")
    if sa:
        lines.append(f"<b>–ö—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥:</b>\n{sa}")
    if ra:
        lines.append(f"<b>–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ:</b>\n{ra}")
    if citations:
        lines.append("<b>–¶–∏—Ç–∞—Ç—ã –∏–∑ –ü—Ä–∞–≤–∏–ª:</b>")
        for c in citations:
            pn = c.get("punkt_num", "")
            sp = c.get("subpunkt_num", "")
            head = f"–ø. {pn}{('.' + sp) if sp else ''}".strip()
            qt = html.escape(c.get("quote", ""))
            lines.append(f"‚Äî <i>{head}</i>:\n{qt}")
    if related:
        lines.append("<b>–°–≤—è–∑–∞–Ω–Ω—ã–µ –ø—É–Ω–∫—Ç—ã:</b>")
        for r in related[:12]:
            pn = html.escape(str(r.get("punkt_num", "")))
            sp = html.escape(str(r.get("subpunkt_num", "")))
            head = f"–ø. {pn}{('.' + sp) if sp else ''}".strip()
            lines.append(f"‚Ä¢ {head}")

    return "\n".join(lines).strip()
def render_related(intent: str, related_items: list[str]) -> str:
    if intent in INTENTS_HIDE_RELATED:
        return ""
    if not related_items:
        return ""
    return "–°–≤—è–∑–∞–Ω–Ω—ã–µ –ø—É–Ω–∫—Ç—ã:\n" + "\n".join(f"‚Ä¢ {x}" for x in related_items)

def tg_set_webhook(full_url: str, secret: Optional[str]) -> None:
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/setWebhook"
    payload = {"url": full_url, "allowed_updates": ["message", "callback_query"], "drop_pending_updates": True}
    if secret:
        payload["secret_token"] = secret
    r = requests.post(url, json=payload, timeout=15)
    if not r.ok:
        logger.error("setWebhook failed: %s %s", r.status_code, r.text)
    else:
        logger.info("Webhook set: %s", full_url)
def tg_answer_callback_query(callback_query_id: str) -> None:
    url = f"{TELEGRAM_API}/answerCallbackQuery"
    payload = {"callback_query_id": callback_query_id}
    r = requests.post(url, json=payload, timeout=15)
    if not r.ok:
        logger.error("answerCallbackQuery failed: %s %s", r.status_code, r.text)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Google Sheets –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def log_to_sheet_safe(user_id: int, question: str, short_answer: str) -> None:
    if not (SHEET_ID and GOOGLE_CREDENTIALS_JSON):
        return
    try:
        import gspread  # type: ignore
        from google.oauth2.service_account import Credentials  # type: ignore
        import json as _json

        creds_data = _json.loads(GOOGLE_CREDENTIALS_JSON)
        scopes = ["https://www.googleapis.com/auth/spreadsheets"]
        creds = Credentials.from_service_account_info(creds_data, scopes=scopes)
        gc = gspread.authorize(creds)
        sh = gc.open_by_key(SHEET_ID)
        ws = sh.sheet1
        ws.append_row([int(time.time()), str(user_id), question, short_answer], value_input_option="USER_ENTERED")
    except Exception as e:
        logger.warning("Sheets log failed: %s", e)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ HTTP —Ö–µ–Ω–¥–ª–µ—Ä—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def handle_health(request: web.Request) -> web.Response:
    ok = (PUNKT_EMBS.ndim == 2 and PUNKT_EMBS.shape[0] == len(PUNKTS))
    payload = {
        "ok": ok,
        "punkts": len(PUNKTS),
        "emb_shape": list(PUNKT_EMBS.shape),
        "bm25": bool(BM25 is not None),
        "cache_size": len(_EMB_CACHE) if "_EMB_CACHE" in globals() else 0,
    }
    return web.json_response(payload)


async def handle_webhook(request: web.Request) -> web.Response:
    if TELEGRAM_WEBHOOK_SECRET:
        recv_secret = request.headers.get("X-Telegram-Bot-Api-Secret-Token")
        if recv_secret != TELEGRAM_WEBHOOK_SECRET:
            logger.warning("Webhook rejected: secret mismatch or missing header. "
                           "Most likely your reverse proxy drops 'X-Telegram-Bot-Api-Secret-Token'. "
                           "Either pass the header through or unset TELEGRAM_WEBHOOK_SECRET.")
            return web.Response(status=403, text="forbidden")

    data = await request.json()
    logger.info("Update keys: %s", list(data.keys()))

    # 1) CallbackQuery (–∫–Ω–æ–ø–∫–∏)
    if "callback_query" in data:
        cq = data["callback_query"]
        chat_id = cq.get("message", {}).get("chat", {}).get("id")
        message_id = cq.get("message", {}).get("message_id")
        action = cq.get("data")
        if not chat_id or not message_id:
            return web.Response(text="ok")

        key = (int(chat_id), int(message_id))
        stash = LAST_RESPONSES.get(key)

        await run_blocking(tg_answer_callback_query, cq.get("id"))

        if not stash:
            await run_blocking(tg_edit_message_text, chat_id, message_id, "–î–∞–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –û—Ç–ø—Ä–∞–≤—å—Ç–µ –≤–æ–ø—Ä–æ—Å –∑–∞–Ω–æ–≤–æ.")
            return web.Response(text="ok")

        if action == "show_detailed":
            detailed = stash["detailed_html"]
            if len(detailed) <= 4000:
                await run_blocking(tg_edit_message_text, chat_id, message_id, detailed, reply_markup=kb_show_short())
            else:
                notice = stash["short_html"] + "\n\n<i>–ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ –Ω–∏–∂–µ.</i>"
                await run_blocking(tg_edit_message_text, chat_id, message_id, notice, reply_markup=kb_show_short())
                for chunk in split_for_telegram(detailed, 4000):
                    await run_blocking(tg_send_message, chat_id, chunk)
        elif action == "show_short":
            await run_blocking(tg_edit_message_text, chat_id, message_id, stash["short_html"], reply_markup=kb_show_detailed())

        return web.Response(text="ok")

    # 2) –û–±—ã—á–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    message = data.get("message", {}) if isinstance(data, dict) else {}
    chat = message.get("chat", {})
    chat_id = chat.get("id")
    text = message.get("text", "") or ""

    if not chat_id:
        return web.Response(text="ok")

    if text.strip().startswith("/start"):
        await run_blocking(tg_send_message, chat_id, "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ –ü—Ä–∞–≤–∏–ª–∞–º –∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏–∏ –ø–µ–¥–∞–≥–æ–≥–æ–≤ ‚Äî —è –æ—Ç–≤–µ—á—É —Å —Ü–∏—Ç–∞—Ç–∞–º–∏.")
        return web.Response(text="ok")

    if not text.strip():
        await run_blocking(tg_send_message, chat_id, "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–∏—à–ª–∏—Ç–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –≤–æ–ø—Ä–æ—Å.")
        return web.Response(text="ok")

    # –ü–µ—Ä-—á–∞—Ç–æ–≤–∞—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞–ø–¥–µ–π—Ç—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
    lock = LOCKS.setdefault(int(chat_id), asyncio.Lock())

    async with lock:
        try:
            # 0) –ò–Ω—Ç–µ–Ω—Ç
            intent_info = classify_question(text)
            logger.info("intent=%s cat=%s", intent_info.get("intent"), intent_info.get("category"))

            # 1) must-have –ø–æ –ø–æ–ª–∏—Ç–∏–∫–µ
            policy_pairs = policy_get_must_have_pairs(intent_info)
            logger.info("policy_pairs=%s", policy_pairs)
            punkts = await run_blocking(rag_search, text, must_have_pairs=policy_pairs)

            # NEW: LLM-rerank + —Å—É–∂–µ–Ω–∏–µ –ø–æ –∏–Ω—Ç–µ–Ω—Ç—É
            punkts = llm_rerank(text, punkts, top_n=18)
            punkts = narrow_punkts_by_intent(text, punkts)

            logger.info("top_punkts=%s", [(p.get('punkt_num'), p.get('subpunkt_num')) for p in punkts[:10]])

            # 2) LLM
            data_struct = await run_blocking(ask_llm, text, punkts)


            # 3) –ü–æ–ª–∏—Ç–∏–∫–∞: –º–∏–Ω–∏–º—É–º —Ü–∏—Ç–∞—Ç –∏ –∫—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç
            data_struct = ensure_min_citations_policy(text, data_struct, punkts, intent_info)
            data_struct = enforce_short_answer_policy(text, data_struct, punkts, intent_info)

            # 4) –ë—É–ª–ª–µ—Ç—ã ‚Äî —Ç–æ–ª—å–∫–æ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            if intent_info.get("intent") == "category_requirements":
                data_struct = enforce_reasoned_answer(text, data_struct, punkts)

            # 4a) –ü–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ
            data_struct = enforce_policy_reasoned_answer(text, data_struct, intent_info)

            # 4b) –°–∞–Ω–∏—Ç–∞–π–∑ —á–∏—Å–µ–ª (–±–ª–æ–∫ #6)
            data_struct = sanitize_numeric_claims(data_struct)

            # 5) HTML –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞
            short_html = render_short_html(text, data_struct)
            detailed_html = render_detailed_html(text, data_struct, punkts)

            msg_id = await run_blocking(tg_send_message, chat_id, short_html, reply_markup=kb_show_detailed())
            if msg_id:
                LAST_RESPONSES[(int(chat_id), int(msg_id))] = {
                    "short_html": short_html,
                    "detailed_html": detailed_html,
                }
        except Exception:
            logger.exception("Processing failed")
            await run_blocking(tg_send_message, chat_id, "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")

    return web.Response(text="ok")
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ main() ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –≤ main():


async def on_startup(app: web.Application):
    full_url = f"{WEBHOOK_URL}{WEBHOOK_PATH}"

    tg_set_webhook(full_url, TELEGRAM_WEBHOOK_SECRET)
    logger.info("Service started on port %s", PORT)


def main():
    app = web.Application()

    app.router.add_get("/health", handle_health)
    app.router.add_post(WEBHOOK_PATH, handle_webhook)
    app.router.add_get("/debug/webhook", handle_debug_webhook)
    async def handle_root(request):
        return web.Response(text="ok")
    app.router.add_get("/", handle_root)

    app.on_startup.append(on_startup)

    loop = asyncio.get_event_loop()
    runner = web.AppRunner(app)
    loop.run_until_complete(runner.setup())
    site = web.TCPSite(runner, "0.0.0.0", PORT)
    loop.run_until_complete(site.start())
    try:
        loop.run_forever()
    except KeyboardInterrupt:
        pass
    finally:
        loop.run_until_complete(runner.cleanup())



if __name__ == "__main__":
    main()
